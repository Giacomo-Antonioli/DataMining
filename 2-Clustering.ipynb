{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--0.0---------------------  SETTINGS -----------------------------------------\n",
    "\"\"\"\n",
    "Data Settings & Importing Libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from math import pi\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "#!conda install --yes --prefix {sys.prefix} plotly\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, silhouette_samples\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import plotly.io as pio\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import mode\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas.plotting import scatter_matrix\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# from pyclustering.cluster import\n",
    "\n",
    "\n",
    "#%matplotlib inline \n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "#plt.rc('figure', figsize=(10, 8))\n",
    "#plt.grid(True)\n",
    "#os.chdir('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\NewDataset')\n",
    "dir = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dropped = pd.read_csv(dir + '\\\\Data\\\\df_dropped.csv')\n",
    "vs_num = pd.read_csv(dir + '\\\\Data\\\\vs_num.csv', index_col=0)\n",
    "vs_cat = pd.read_csv(dir + '\\\\Data\\\\vs_cat.csv', index_col=0)\n",
    "bvs_num = pd.read_csv(dir + '\\\\Data\\\\bvs_num.csv', index_col=0)\n",
    "bvs_cat = pd.read_csv(dir + '\\\\Data\\\\bvs_cat.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2675241 entries, 0 to 2675240\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Dtype  \n",
      "---  ------           -----  \n",
      " 0   id_ram           int64  \n",
      " 1   time_code        object \n",
      " 2   sales_usd        float64\n",
      " 3   brand            object \n",
      " 4   ram_model        object \n",
      " 5   memory_dim       float64\n",
      " 6   memory_type      object \n",
      " 7   clock            int64  \n",
      " 8   vendor           object \n",
      " 9   continent        object \n",
      " 10  country          object \n",
      " 11  region           object \n",
      " 12  currency         object \n",
      " 13  conversion_rate  float64\n",
      "dtypes: float64(3), int64(2), object(9)\n",
      "memory usage: 285.7+ MB\n",
      "---------------------------------------------------------------\n",
      "           id_ram   time_code  sales_usd    brand          ram_model  \\\n",
      "count  2675241.00     2675241 2675241.00  2675241            2675241   \n",
      "unique        nan        1820        nan       45                272   \n",
      "top           nan  2016-04-11        nan  G.SKILL  Corsair Vengeance   \n",
      "freq          nan        4465        nan   691263             229079   \n",
      "mean      5252.08         NaN     152.60      NaN                NaN   \n",
      "std        840.98         NaN     113.98      NaN                NaN   \n",
      "min       3720.00         NaN      34.46      NaN                NaN   \n",
      "25%       4535.00         NaN      67.10      NaN                NaN   \n",
      "50%       5130.00         NaN     113.72      NaN                NaN   \n",
      "75%       5940.00         NaN     202.54      NaN                NaN   \n",
      "max       7422.00         NaN     544.55      NaN                NaN   \n",
      "\n",
      "        memory_dim memory_type      clock            vendor continent  \\\n",
      "count   2675241.00     2675241 2675241.00           2675241   2675241   \n",
      "unique         nan           7        nan                78         3   \n",
      "top            nan        DDR3        nan  geizhals_unknown    Europe   \n",
      "freq           nan     1216874        nan           1504132   2228594   \n",
      "mean         14.94         NaN    1988.24               NaN       NaN   \n",
      "std          12.07         NaN     742.99               NaN       NaN   \n",
      "min           0.25         NaN     133.00               NaN       NaN   \n",
      "25%           8.00         NaN    1600.00               NaN       NaN   \n",
      "50%          12.00         NaN    1866.00               NaN       NaN   \n",
      "75%          16.00         NaN    2400.00               NaN       NaN   \n",
      "max         128.00         NaN    4600.00               NaN       NaN   \n",
      "\n",
      "        country              region currency  conversion_rate  \n",
      "count   2675241             2675241  2675241       2675241.00  \n",
      "unique       11                  75        6              nan  \n",
      "top     Germany  schleswig-holstein      EUR              nan  \n",
      "freq    1652552              109090  1797886              nan  \n",
      "mean        NaN                 NaN      NaN             1.15  \n",
      "std         NaN                 NaN      NaN             0.18  \n",
      "min         NaN                 NaN      NaN             0.67  \n",
      "25%         NaN                 NaN      NaN             1.07  \n",
      "50%         NaN                 NaN      NaN             1.14  \n",
      "75%         NaN                 NaN      NaN             1.27  \n",
      "max         NaN                 NaN      NaN             1.59  \n",
      "---------------------------------------------------------------\n",
      "   id_ram   time_code  sales_usd  brand ram_model  memory_dim memory_type  \\\n",
      "0    3720  2014-07-14      34.89  ADATA     Adata        1.00         DDR   \n",
      "1    3720  2014-07-15      34.92  ADATA     Adata        1.00         DDR   \n",
      "2    3720  2014-07-16      34.83  ADATA     Adata        1.00         DDR   \n",
      "3    3720  2014-07-17      34.73  ADATA     Adata        1.00         DDR   \n",
      "4    3720  2014-07-18      34.58  ADATA     Adata        1.00         DDR   \n",
      "\n",
      "   clock            vendor continent  country                region currency  \\\n",
      "0    333  geizhals_unknown    Europe  Germany  rhineland-palatinate      EUR   \n",
      "1    333  geizhals_unknown    Europe  Germany             thuringia      EUR   \n",
      "2    333  geizhals_unknown    Europe  Germany               bavaria      EUR   \n",
      "3    333  geizhals_unknown    Europe    Spain              analucia      EUR   \n",
      "4    333  geizhals_unknown    Europe  Germany                bremen      EUR   \n",
      "\n",
      "   conversion_rate  \n",
      "0             1.36  \n",
      "1             1.36  \n",
      "2             1.36  \n",
      "3             1.35  \n",
      "4             1.35  \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_dropped.info()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(df_dropped.describe(include='all'))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(df_dropped.head())\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43 entries, OutletPC to Senetic\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   sales_Europe   43 non-null     float64\n",
      " 1   sales_Oceania  43 non-null     float64\n",
      " 2   sales_America  43 non-null     float64\n",
      " 3   Imin           43 non-null     int64  \n",
      " 4   Ew             43 non-null     float64\n",
      " 5   Cont_Max       43 non-null     int64  \n",
      " 6   Dim_unik       43 non-null     int64  \n",
      " 7   Type_unik      43 non-null     int64  \n",
      " 8   coun_unik      43 non-null     int64  \n",
      " 9   reg_unik       43 non-null     int64  \n",
      " 10  price_unik     43 non-null     int64  \n",
      " 11  Rate_avg       43 non-null     float64\n",
      "dtypes: float64(5), int64(7)\n",
      "memory usage: 4.4+ KB\n",
      "---------------------------------------------------------------\n",
      "       sales_Europe  sales_Oceania  sales_America  Imin    Ew  Cont_Max  \\\n",
      "count         43.00          43.00          43.00 43.00 43.00     43.00   \n",
      "mean         553.21         401.33         796.58  1.44  2.48      1.65   \n",
      "std         1037.83         697.13        1714.29  1.40  0.26      0.48   \n",
      "min            0.00           0.00           0.00  1.00  1.58      1.00   \n",
      "25%           12.00           0.00           0.00  1.00  2.34      1.00   \n",
      "50%          120.00          58.00           0.00  1.00  2.54      2.00   \n",
      "75%          459.50         486.50         204.50  1.00  2.69      2.00   \n",
      "max         4180.00        2549.00        8060.00  7.00  2.79      2.00   \n",
      "\n",
      "       Dim_unik  Type_unik  coun_unik  reg_unik  price_unik  Rate_avg  \n",
      "count     43.00      43.00      43.00     43.00       43.00     43.00  \n",
      "mean       5.58       2.42       1.81      8.40     1158.67      1.00  \n",
      "std        1.99       0.54       0.55      2.99     1193.72      0.25  \n",
      "min        3.00       1.00       1.00      3.00       18.00      0.70  \n",
      "25%        4.00       2.00       2.00      7.00      265.00      0.76  \n",
      "50%        5.00       2.00       2.00      7.00      718.00      1.00  \n",
      "75%        6.00       3.00       2.00     11.00     1752.00      1.22  \n",
      "max       11.00       3.00       4.00     16.00     4568.00      1.41  \n",
      "---------------------------------------------------------------\n",
      "                  sales_Europe  sales_Oceania  sales_America  Imin   Ew  \\\n",
      "vendor                                                                    \n",
      "OutletPC                  0.00           0.00        8060.00     1 2.45   \n",
      "Alternate Italia        161.00          73.00           0.00     2 2.26   \n",
      "Adorama                   0.00           0.00        3334.00     1 2.69   \n",
      "Mighty Ape               50.00         279.00           0.00     1 2.40   \n",
      "Dell                      0.00           0.00         803.00     1 2.77   \n",
      "\n",
      "                  Cont_Max  Dim_unik  Type_unik  coun_unik  reg_unik  \\\n",
      "vendor                                                                 \n",
      "OutletPC                 1         9          3          1         7   \n",
      "Alternate Italia         2         6          3          2         3   \n",
      "Adorama                  1        11          3          1         7   \n",
      "Mighty Ape               2         4          2          4        14   \n",
      "Dell                     1         7          3          1         7   \n",
      "\n",
      "                  price_unik  Rate_avg  \n",
      "vendor                                  \n",
      "OutletPC                2417      1.00  \n",
      "Alternate Italia         200      1.23  \n",
      "Adorama                 1506      1.00  \n",
      "Mighty Ape               312      0.76  \n",
      "Dell                     109      1.00  \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vs_num.info()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(vs_num.describe(include='all'))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(vs_num.head())\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 43 entries, OutletPC to Senetic\n",
      "Data columns (total 15 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Country        43 non-null     object \n",
      " 1   Fav_weekday    43 non-null     object \n",
      " 2   Fav_month      43 non-null     object \n",
      " 3   fav_dim        43 non-null     float64\n",
      " 4   fav_price      43 non-null     float64\n",
      " 5   fav_model      43 non-null     object \n",
      " 6   fav_mem_type   43 non-null     object \n",
      " 7   fav_clock      43 non-null     int64  \n",
      " 8   fav_continent  43 non-null     object \n",
      " 9   fav_region     43 non-null     object \n",
      " 10  Conv_most      43 non-null     float64\n",
      " 11  Conv_less      43 non-null     float64\n",
      " 12  Conv_max       43 non-null     float64\n",
      " 13  Conv_min       43 non-null     float64\n",
      " 14  fav_week       43 non-null     int64  \n",
      "dtypes: float64(6), int64(2), object(7)\n",
      "memory usage: 5.4+ KB\n",
      "---------------------------------------------------------------\n",
      "          Country Fav_weekday Fav_month  fav_dim  fav_price  \\\n",
      "count          43          43        43    43.00      43.00   \n",
      "unique         10           2         5      nan        nan   \n",
      "top     Australia      Friday     April      nan        nan   \n",
      "freq           10          42        26      nan        nan   \n",
      "mean          NaN         NaN       NaN    13.40      83.03   \n",
      "std           NaN         NaN       NaN     4.08      46.78   \n",
      "min           NaN         NaN       NaN     4.00      34.95   \n",
      "25%           NaN         NaN       NaN     8.00      46.88   \n",
      "50%           NaN         NaN       NaN    16.00      69.89   \n",
      "75%           NaN         NaN       NaN    16.00     114.94   \n",
      "max           NaN         NaN       NaN    16.00     232.06   \n",
      "\n",
      "                fav_model fav_mem_type  fav_clock fav_continent  \\\n",
      "count                  43           43      43.00            43   \n",
      "unique                 10            3        nan             3   \n",
      "top     Corsair Vengeance         DDR4        nan       Oceania   \n",
      "freq                   23           29        nan            16   \n",
      "mean                  NaN          NaN    2037.14           NaN   \n",
      "std                   NaN          NaN     526.17           NaN   \n",
      "min                   NaN          NaN     800.00           NaN   \n",
      "25%                   NaN          NaN    1600.00           NaN   \n",
      "50%                   NaN          NaN    2133.00           NaN   \n",
      "75%                   NaN          NaN    2400.00           NaN   \n",
      "max                   NaN          NaN    3200.00           NaN   \n",
      "\n",
      "            fav_region  Conv_most  Conv_less  Conv_max  Conv_min  fav_week  \n",
      "count               43      43.00      43.00     43.00     43.00     43.00  \n",
      "unique              29        nan        nan       nan       nan       nan  \n",
      "top     north-east usa        nan        nan       nan       nan       nan  \n",
      "freq                 2        nan        nan       nan       nan       nan  \n",
      "mean               NaN       0.99       0.97      1.05      0.97     15.95  \n",
      "std                NaN       0.24       0.24      0.27      0.24     12.42  \n",
      "min                NaN       0.68       0.67      0.73      0.67      1.00  \n",
      "25%                NaN       0.76       0.72      0.81      0.72      8.00  \n",
      "50%                NaN       1.00       1.00      1.00      1.00     13.00  \n",
      "75%                NaN       1.20       1.18      1.25      1.18     17.00  \n",
      "max                NaN       1.40       1.40      1.48      1.40     51.00  \n",
      "---------------------------------------------------------------\n",
      "                                   Country Fav_weekday Fav_month  fav_dim  \\\n",
      "vendor                                                                      \n",
      "OutletPC          United States of America      Friday     April    16.00   \n",
      "Alternate Italia                     Italy      Friday     March    16.00   \n",
      "Adorama           United States of America      Friday     April     8.00   \n",
      "Mighty Ape                       Australia      Friday  February    16.00   \n",
      "Dell              United States of America      Friday     April     4.00   \n",
      "\n",
      "                  fav_price          fav_model fav_mem_type  fav_clock  \\\n",
      "vendor                                                                   \n",
      "OutletPC              69.89  Corsair Vengeance         DDR4       1600   \n",
      "Alternate Italia      46.03  G.Skill Trident Z         DDR3       1600   \n",
      "Adorama               79.99            Crucial         DDR4       1600   \n",
      "Mighty Ape           131.38  Corsair Vengeance         DDR4       2400   \n",
      "Dell                  89.99          Visiontek         DDR2        800   \n",
      "\n",
      "                 fav_continent      fav_region  Conv_most  Conv_less  \\\n",
      "vendor                                                                 \n",
      "OutletPC               America    mid-atalntic       1.00       1.00   \n",
      "Alternate Italia        Europe     north italy       1.23       1.23   \n",
      "Adorama                America  south-east usa       1.00       1.00   \n",
      "Mighty Ape             Oceania        brussels       0.72       0.72   \n",
      "Dell                   America        west usa       1.00       1.00   \n",
      "\n",
      "                  Conv_max  Conv_min  fav_week  \n",
      "vendor                                          \n",
      "OutletPC              1.00      1.00        14  \n",
      "Alternate Italia      1.24      1.23        11  \n",
      "Adorama               1.00      1.00        10  \n",
      "Mighty Ape            0.79      0.72        13  \n",
      "Dell                  1.00      1.00        16  \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vs_cat.info()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(vs_cat.describe(include='all'))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(vs_cat.head())\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 231 entries, pricespy_unknown to Senetic\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Unnamed: 1     231 non-null    object \n",
      " 1   sales_Europe   231 non-null    float64\n",
      " 2   sales_Oceania  231 non-null    float64\n",
      " 3   sales_America  231 non-null    float64\n",
      " 4   Imin           231 non-null    int64  \n",
      " 5   Iavg           231 non-null    float64\n",
      " 6   Type_unik      231 non-null    float64\n",
      " 7   reg_unik       231 non-null    float64\n",
      " 8   Rate_avg       231 non-null    float64\n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 18.0+ KB\n",
      "---------------------------------------------------------------\n",
      "       Unnamed: 1  sales_Europe  sales_Oceania  sales_America   Imin   Iavg  \\\n",
      "count         231        231.00         231.00         231.00 231.00 231.00   \n",
      "unique         31           nan            nan            nan    nan    nan   \n",
      "top       CRUCIAL           nan            nan            nan    nan    nan   \n",
      "freq           22           nan            nan            nan    nan    nan   \n",
      "mean          NaN         14.06           6.16          14.21   1.04   1.34   \n",
      "std           NaN         29.19          13.55          33.85   0.20   0.53   \n",
      "min           NaN          0.00           0.00           0.00   1.00   1.00   \n",
      "25%           NaN          0.00           0.00           0.00   1.00   1.00   \n",
      "50%           NaN          3.00           0.00           0.00   1.00   1.09   \n",
      "75%           NaN         10.00           6.00           5.50   1.00   1.50   \n",
      "max           NaN        145.00          73.00         218.00   2.00   4.50   \n",
      "\n",
      "        Type_unik  reg_unik  Rate_avg  \n",
      "count      231.00    231.00    231.00  \n",
      "unique        nan       nan       nan  \n",
      "top           nan       nan       nan  \n",
      "freq          nan       nan       nan  \n",
      "mean         1.28      5.94      1.01  \n",
      "std          0.49      3.09      0.24  \n",
      "min          1.00      1.00      0.70  \n",
      "25%          1.00      4.00      0.77  \n",
      "50%          1.00      6.00      1.00  \n",
      "75%          2.00      7.00      1.28  \n",
      "max          3.00     16.00      1.43  \n",
      "---------------------------------------------------------------\n",
      "                 Unnamed: 1  sales_Europe  sales_Oceania  sales_America  Imin  \\\n",
      "pricespy_unknown       AFOX          7.00           0.00           0.00     2   \n",
      "pricespy_unknown        AMD         18.00           0.00           0.00     1   \n",
      "pricespy_unknown     ELIXIR          9.00           0.00           0.00     1   \n",
      "pricespy_unknown        OCZ          4.00           0.00           0.00     1   \n",
      "pricespy_unknown     PANRAM         92.00           3.00           0.00     1   \n",
      "\n",
      "                  Iavg  Type_unik  reg_unik  Rate_avg  \n",
      "pricespy_unknown  2.33       1.00      5.00      1.39  \n",
      "pricespy_unknown  1.12       1.00      9.00      1.40  \n",
      "pricespy_unknown  1.00       1.00      5.00      1.34  \n",
      "pricespy_unknown  1.00       1.00      2.00      1.36  \n",
      "pricespy_unknown  1.16       2.00     11.00      1.36  \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bvs_num.info()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(bvs_num.describe(include='all'))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(bvs_num.head())\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 231 entries, pricespy_unknown to Senetic\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Unnamed: 1     231 non-null    object \n",
      " 1   fav_country    231 non-null    object \n",
      " 2   Fav_weekday    231 non-null    object \n",
      " 3   Fav_month      231 non-null    object \n",
      " 4   fav_dim        231 non-null    float64\n",
      " 5   fav_price      231 non-null    float64\n",
      " 6   fav_model      231 non-null    object \n",
      " 7   fav_mem_type   231 non-null    object \n",
      " 8   fav_clock      231 non-null    float64\n",
      " 9   fav_continent  231 non-null    object \n",
      " 10  fav_region     231 non-null    object \n",
      " 11  Conv_most      231 non-null    float64\n",
      " 12  Conv_less      231 non-null    float64\n",
      " 13  Conv_max       231 non-null    float64\n",
      " 14  Conv_min       231 non-null    float64\n",
      " 15  fav_week       231 non-null    float64\n",
      "dtypes: float64(8), object(8)\n",
      "memory usage: 30.7+ KB\n",
      "---------------------------------------------------------------\n",
      "       Unnamed: 1               fav_country Fav_weekday Fav_month  fav_dim  \\\n",
      "count         231                       231         231       231   231.00   \n",
      "unique         31                         9           7        12      nan   \n",
      "top       CRUCIAL  United States of America     Tuesday     March      nan   \n",
      "freq           22                        64          60        37      nan   \n",
      "mean          NaN                       NaN         NaN       NaN    10.00   \n",
      "std           NaN                       NaN         NaN       NaN     6.75   \n",
      "min           NaN                       NaN         NaN       NaN     2.00   \n",
      "25%           NaN                       NaN         NaN       NaN     8.00   \n",
      "50%           NaN                       NaN         NaN       NaN     8.00   \n",
      "75%           NaN                       NaN         NaN       NaN    16.00   \n",
      "max           NaN                       NaN         NaN       NaN    64.00   \n",
      "\n",
      "        fav_price fav_model fav_mem_type  fav_clock fav_continent  \\\n",
      "count      231.00       231          231     231.00           231   \n",
      "unique        nan        76            3        nan             3   \n",
      "top           nan        Hp         DDR3        nan       America   \n",
      "freq          nan        18          143        nan            96   \n",
      "mean        96.78       NaN          NaN    1903.48           NaN   \n",
      "std         71.59       NaN          NaN     527.92           NaN   \n",
      "min         34.47       NaN          NaN     800.00           NaN   \n",
      "25%         47.56       NaN          NaN    1600.00           NaN   \n",
      "50%         70.37       NaN          NaN    1600.00           NaN   \n",
      "75%        117.92       NaN          NaN    2400.00           NaN   \n",
      "max        476.66       NaN          NaN    3200.00           NaN   \n",
      "\n",
      "          fav_region  Conv_most  Conv_less  Conv_max  Conv_min  fav_week  \n",
      "count            231     231.00     231.00    231.00    231.00    231.00  \n",
      "unique            50        nan        nan       nan       nan       nan  \n",
      "top     mid-atalntic        nan        nan       nan       nan       nan  \n",
      "freq              16        nan        nan       nan       nan       nan  \n",
      "mean             NaN       0.99       0.99      1.05      0.99     20.93  \n",
      "std              NaN       0.23       0.23      0.27      0.23     15.46  \n",
      "min              NaN       0.67       0.67      0.71      0.67      1.00  \n",
      "25%              NaN       0.75       0.74      0.79      0.74      8.50  \n",
      "50%              NaN       1.00       1.00      1.00      1.00     15.00  \n",
      "75%              NaN       1.22       1.22      1.34      1.22     36.00  \n",
      "max              NaN       1.42       1.42      1.54      1.42     52.00  \n",
      "---------------------------------------------------------------\n",
      "                 Unnamed: 1     fav_country Fav_weekday  Fav_month  fav_dim  \\\n",
      "pricespy_unknown       AFOX  United Kingdom      Monday      March     8.00   \n",
      "pricespy_unknown        AMD  United Kingdom      Monday    January     8.00   \n",
      "pricespy_unknown     ELIXIR  United Kingdom   Wednesday   December     4.00   \n",
      "pricespy_unknown        OCZ  United Kingdom      Monday  September     4.00   \n",
      "pricespy_unknown     PANRAM  United Kingdom      Sunday       July     8.00   \n",
      "\n",
      "                  fav_price             fav_model fav_mem_type  fav_clock  \\\n",
      "pricespy_unknown      56.43                  Afox         DDR4    2400.00   \n",
      "pricespy_unknown     104.88    Amd R7 Performance         DDR3    1866.00   \n",
      "pricespy_unknown      39.07                Elixir         DDR3    1600.00   \n",
      "pricespy_unknown     432.44      Ocz Obsidian Xtc         DDR3    1600.00   \n",
      "pricespy_unknown      36.14  Panram Ninja-V White         DDR4    2400.00   \n",
      "\n",
      "                 fav_continent          fav_region  Conv_most  Conv_less  \\\n",
      "pricespy_unknown        Europe  south west england       1.39       1.39   \n",
      "pricespy_unknown        Europe       east midlands       1.33       1.33   \n",
      "pricespy_unknown        Europe  south west england       1.33       1.33   \n",
      "pricespy_unknown        Europe              london       1.25       1.25   \n",
      "pricespy_unknown        Europe              london       1.28       1.28   \n",
      "\n",
      "                  Conv_max  Conv_min  fav_week  \n",
      "pricespy_unknown      1.40      1.39     11.00  \n",
      "pricespy_unknown      1.54      1.33      1.00  \n",
      "pricespy_unknown      1.36      1.33      1.00  \n",
      "pricespy_unknown      1.50      1.25      3.00  \n",
      "pricespy_unknown      1.47      1.28      9.00  \n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bvs_cat.info()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(bvs_cat.describe(include='all'))\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(bvs_cat.head())\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- df_dropped dataset\n",
    "# conversion \n",
    "df_dropped['id_ram'] = df_dropped['id_ram'].astype(object)\n",
    "df_dropped['memory_dim'] = df_dropped['memory_dim'].astype(object)\n",
    "df_dropped['clock'] = df_dropped['clock'].astype(object)\n",
    "df_dropped['time_code'] = pd.to_datetime(df_dropped['time_code'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "#   Attributes split by type\n",
    "num_float = ['sales_usd', 'conversion_rate']\n",
    "cat = [\"clock\", \"memory_dim\", 'id_ram', 'time_code', 'brand', 'ram_model', 'memory_type', 'vendor', 'continent', 'country', 'region', \"currency\"]\n",
    "\n",
    "#   per vedere i valori distinti - USARE ATTRIBUTI CATEGORICI\n",
    "for col in cat: \n",
    "    print(\"\\nDistinct values in \" + col + \" : \\t\", df_dropped[col].unique())\n",
    "\n",
    "\n",
    "#-------dropping\n",
    "df_dropped.drop(\"Unnamed: 0\", inplace=True, axis=1)\n",
    "bvs_num.drop(\"Unnamed: 1\", inplace=True, axis=1)\n",
    "bvs_cat.drop(\"Unnamed: 1\", inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# --------------------------- \n",
    "# conversion \n",
    "for x in vs_num:\n",
    "    vs_num[x] = vs_num[x].astype(float)\n",
    "\n",
    "for x in bvs_num:\n",
    "    bvs_num[x] = bvs_num[x].astype(float)\n",
    "\n",
    "for x in vs_cat:\n",
    "    vs_cat[x] = vs_cat[x].astype(object)\n",
    "\n",
    "for x in bvs_cat:\n",
    "    bvs_cat[x] = bvs_cat[x].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "temp_list = vs_num.columns\n",
    "vs_num_norm = vs_num.copy()\n",
    "vs_num_norm[temp_list] = scaler.fit_transform(vs_num[temp_list].values)\n",
    "\n",
    "scaler_bvs = MinMaxScaler()\n",
    "temp_list = bvs_num.columns\n",
    "bvs_num_norm = bvs_num.copy()\n",
    "bvs_num_norm[temp_list] = scaler_bvs.fit_transform(bvs_num[temp_list].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(vs_num_norm)\n",
    "\n",
    "print(len(pca.components_))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 7]\n",
    "plt.step(range(1, vs_num.shape[1] + 1), pca.explained_variance_ratio_.cumsum(),\n",
    "    where='mid', label='Cumulative Explained Variance')\n",
    "plt.bar(range(1, vs_num.shape[1] + 1), pca.explained_variance_ratio_,\n",
    "    alpha=0.4, color='g', label='Individual Explained Variance')\n",
    "plt.title('vs_num')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc='center right');\n",
    "#plt.savefig('../tex/img/clustering/pca.png')\n",
    "plt.show()\n",
    "\n",
    "pca_bvs = PCA()\n",
    "pca_bvs.fit(bvs_num_norm)\n",
    "\n",
    "plt.step(range(1, bvs_num.shape[1] + 1), pca_bvs.explained_variance_ratio_.cumsum(),\n",
    "    where='mid', label='Cumulative Explained Variance')\n",
    "plt.bar(range(1, bvs_num.shape[1] + 1), pca_bvs.explained_variance_ratio_,\n",
    "    alpha=0.4, color='g', label='Individual Explained Variance')\n",
    "plt.title('bvs_num')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc='center right');\n",
    "#plt.savefig('../tex/img/clustering/pca.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rule of thumb is to preserve around 80% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca_vs_num_norm = pca.fit_transform(vs_num_norm)\n",
    "#print(pca_vs_num_norm.shape)\n",
    "#print(pca.inverse_transform(pca_vs_num_norm).shape)\n",
    "\n",
    "pca_bvs = PCA(n_components=4)\n",
    "pca_bvs_num_norm = pca_bvs.fit_transform(bvs_num_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "def plot_silhouette_score(df, labels):\n",
    "    cluster_labels = np.unique(labels)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_samples(df, labels)\n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    y_tick = []\n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        c_silhouette_vals = silhouette_vals[labels == c]\n",
    "        c_silhouette_vals.sort()\n",
    "        print('Cluster', c, 'avg silhouette:', np.mean(c_silhouette_vals))\n",
    "        y_ax_upper += len(c_silhouette_vals)\n",
    "        color = cm.jet(float(i)/n_clusters)\n",
    "        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "        y_tick.append((y_ax_lower + y_ax_upper) / 2.)\n",
    "        y_ax_lower += len(c_silhouette_vals)\n",
    "        \n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    print('Total avg silhouette:', silhouette_avg)\n",
    "    plt.axvline(silhouette_avg, color='red', linestyle='--')\n",
    "    plt.yticks=(y_tick, cluster_labels + 1)\n",
    "    plt.ylabel('Points')\n",
    "    plt.xlabel('Silhouette')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_basic_statistics(df, labels, columns):\n",
    "    df = pd.DataFrame(df, columns=columns)\n",
    "    df2 = df.copy()\n",
    "    df2['cluster'] = labels\n",
    "    columns = df2.columns[0:-1]\n",
    "    means = []\n",
    "    std = []\n",
    "    for i in np.unique(df2['cluster'].array):\n",
    "        clust = df2.loc[df2['cluster'] == i, columns]\n",
    "        means.append(clust.describe().loc['mean'].values)\n",
    "        std.append(clust.describe().loc['std'].values)\n",
    "        legend = []\n",
    "    for i in np.unique(df2['cluster'].array):\n",
    "        plt.plot(range(0, columns.shape[0]), means[i], marker='o')\n",
    "        legend.append('Cluster %d' %i)\n",
    "    plt.legend(legend)\n",
    "    plt.xticks(range(0, columns.shape[0]), columns)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for the vendor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = []\n",
    "SIL = []\n",
    "SEP = []\n",
    "CAL = []\n",
    "\n",
    "for k in range(2, 20):        \n",
    "    k_means = cluster.KMeans(n_clusters=k, max_iter=100)   # , random_state=1\n",
    "    clusters = k_means.fit_predict(pca_vs_num_norm)\n",
    "    #k_means = k_means.fit(pca_vs_num_norm)\n",
    "    #p_k_means = k_means.predict(pca_vs_num_norm)\n",
    "    #labels = k_means.labels_\n",
    "    centroids = k_means.cluster_centers_\n",
    "\n",
    "    SSE.append(k_means.inertia_)\n",
    "    SIL.append(silhouette_score(pca_vs_num_norm, clusters, metric = 'euclidean'))\n",
    "    print(\"For n_clusters =\", k,\n",
    "          \"The average silhouette_score is :\", silhouette_score(pca_vs_num_norm, clusters, metric = 'euclidean'))    \n",
    "    SEP.append(davies_bouldin_score(pca_vs_num_norm, clusters))\n",
    "    CAL.append(calinski_harabasz_score(pca_vs_num_norm, clusters))\n",
    "    \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 15]\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "fig.suptitle('Clustring Evaluation')\n",
    "\n",
    "    \n",
    "ax1.plot(range(2, len(SSE)+2), SSE,'o-')\n",
    "ax1.set_title(\"SSE\")\n",
    "ax1.set_xlabel('Number of Clusters')\n",
    "ax1.set_ylabel('SSE')\n",
    "ax1.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "for x,y in zip(range(2, len(SSE)+2), SSE):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax1.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "#ax1.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\sse.jpg\")\n",
    "#ax1.show()\n",
    "\n",
    "ax2.plot(range(2, len(SIL)+2), SIL,'o-')\n",
    "ax2.set_title(\"Silhouette Score\")\n",
    "ax2.set_xlabel('Number of Clusters')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "#ax2.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\silhouette.jpg\")\n",
    "#ax2.show()\n",
    "for x,y in zip(range(2, len(SIL)+2), SIL):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax2.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "ax3.plot(range(2, len(SEP)+2), SEP,'o-')\n",
    "ax3.set_title(\"Davies Bouldin Index\")\n",
    "ax3.set_xlabel('Number of Clusters')\n",
    "ax3.set_ylabel('Davies Bouldin Index')\n",
    "ax3.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "#ax3.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\davies-bouldin.jpg\")\n",
    "#ax3.show()\n",
    "for x,y in zip(range(2, len(SEP)+2), SEP):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax3.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "ax4.plot(range(2, len(CAL)+2), CAL,'o-')\n",
    "ax4.set_title(\"Calinski-Harabasz Index\")\n",
    "ax4.set_xlabel('Number of Clusters')\n",
    "ax4.set_ylabel('Calinski-Harabasz Index')\n",
    "ax4.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "#ax4.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\calinski.jpg\")\n",
    "#ax4.show()\n",
    "for x,y in zip(range(2, len(CAL)+2), CAL):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax4.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [30, 10]\n",
    "plt.title(\"Hierarchical Clustering of the Customers\")\n",
    "plt.axhline(y=80, ls='--', c='red')\n",
    "dend = dendrogram(linkage(vs_num_norm, method='ward'), leaf_rotation=90, leaf_font_size = 8,show_contracted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroids analysis with AffinityPropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "affinity = AffinityPropagation()\n",
    "clustering = affinity.fit_predict(pca_vs_num_norm)\n",
    "#k_means = k_means.fit_predict(vs_num_norm)\n",
    "#p_k_means = k_means.predict(vs_num_norm)\n",
    "#labels = clustering.labels_\n",
    "centroids = affinity.cluster_centers_\n",
    "print('labels: '+str(clustering)) \n",
    "print('labels len: '+str(len(clustering))) \n",
    "print(np.unique(clustering, return_counts=True))\n",
    "\n",
    "\n",
    "# Visualization of centers in 2D\n",
    "centers = pca.inverse_transform(centroids)\n",
    "centers = scaler.inverse_transform(centers)\n",
    "con=1\n",
    "#plt.rcParams[\"figure.figsize\"] = [60, 20]\n",
    "# fig = plt.figure()\n",
    "# print(labels)\n",
    "# color_labels=[]\n",
    "# for x in labels:\n",
    "#     if x==0:\n",
    "#         color_labels.append('red')\n",
    "#     if x==1:\n",
    "#         color_labels.append('blue')\n",
    "#     if x==2:\n",
    "#         color_labels.append('green')\n",
    "#     if x==3:\n",
    "#         color_labels.append('black')\n",
    "#     if x==4:\n",
    "#         color_labels.append('brown')\n",
    "#     if x==5:\n",
    "#         color_labels.append('purple')\n",
    "#     if x==6:\n",
    "#         color_labels.append('yellow')        \n",
    "#     if x==7:\n",
    "#         color_labels.append('orange')\n",
    "#     if x==8:\n",
    "#         color_labels.append('gray')        \n",
    "# for index1, x in enumerate(vs_num_norm.columns):\n",
    "#     for index2, y in enumerate(vs_num_norm.columns):\n",
    "#         if x != y:\n",
    "#             ax = fig.add_subplot(15, 2, con)\n",
    "#             ax.scatter(vs_num[x], vs_num[y], c=color_labels, s=20)\n",
    "#             ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"green\")\n",
    "#             #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "#             ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "#             con+=1\n",
    "# fig.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "cluster_centers_indices = affinity.cluster_centers_indices_\n",
    "\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "print(n_clusters_)\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "\n",
    "#####https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "# for k, col in zip(range(n_clusters_), colors):\n",
    "#     class_members = labels == k\n",
    "#     cluster_center = tmp[cluster_centers_indices[k]]\n",
    "#     plt.plot(tmp[class_members, 0], tmp[class_members, 1], col + '.')\n",
    "#     plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "#              markeredgecolor='k', markersize=14)\n",
    "#     for x in tmp[class_members]:\n",
    "#         plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "# plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "con=1\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 350]\n",
    "fig = plt.figure()\n",
    "for index1, x in enumerate(vs_num_norm.columns):\n",
    "    for index2, y in enumerate(vs_num_norm.columns):\n",
    "        if x != y:\n",
    "           \n",
    "\n",
    "            tmp=vs_num_norm[[x,y]]\n",
    "            \n",
    "            tmp=tmp.to_numpy()\n",
    "            ax = fig.add_subplot((len(vs_num_norm.columns)*(len(vs_num_norm.columns)-1))/3, 3, con)\n",
    "            for k, col in zip(range(n_clusters_), colors):\n",
    "                class_members = clustering == k\n",
    "                cluster_center = tmp[cluster_centers_indices[k]]\n",
    "                \n",
    "                ax.plot(tmp[class_members, 0], tmp[class_members, 1], col + '.')\n",
    "                ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "                         markeredgecolor='k', markersize=14)\n",
    "                for z in tmp[class_members]:\n",
    "                    ax.plot([cluster_center[0], z[0]], [cluster_center[1], z[1]], col)\n",
    "#             ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            ax.set_title(\"Estimated number of clusters: \" +str(n_clusters_)+\" visualization on \" + x + \" and \" + y)\n",
    "            con+=1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster indication:\n",
    "SSE= 3...5\n",
    "DB= 4-7-8\n",
    "CAL= 5...9\n",
    "SIL= 4-5, 7...9\n",
    "\n",
    "Dendrogram = 2\n",
    "\n",
    "Affinity= >= 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALORI DI CLUSTERING = 4 e 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- K = 4 --------------------\n",
    "k_means = cluster.KMeans(n_clusters=4, max_iter=100)   \n",
    "clusters = k_means.fit_predict(pca_vs_num_norm)\n",
    "#k_means = k_means.fit(vs_num_norm)\n",
    "#p_k_means = k_means.predict(vs_num_norm)\n",
    "#labels = k_means.labels_\n",
    "centroids = k_means.cluster_centers_\n",
    "#print(centroids)\n",
    "        \n",
    "print(np.unique(clusters, return_counts=True))\n",
    "\n",
    "\"\"\"for x in pca_vs_num_norm.columns:\n",
    "    for y in pca_vs_num_norm.columns:\n",
    "        for z in pca_vs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                plt.figure(figsize=(20, 15))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=pca_vs_num_norm[x],\n",
    "                    y=pca_vs_num_norm[y],\n",
    "                    z=pca_vs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=labels, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                                scene = dict(\n",
    "                                    xaxis_title= x,\n",
    "                                    yaxis_title= y,\n",
    "                                    zaxis_title= z)\n",
    "                                )\n",
    "                #fig.write_image(dir + \"\\\\Clustering\\\\KMeans\\\\3d-after-kmeans.jpg\")\n",
    "                fig.show()\"\"\"\n",
    "\n",
    "\n",
    "# Visualization of centers in 2D\n",
    "print(centroids.shape)\n",
    "centers = pca.inverse_transform(centroids)\n",
    "print(centers.shape)\n",
    "scaled_centers = centers\n",
    "centers = scaler.inverse_transform(centers)\n",
    "print(centers.shape)\n",
    "#centers = centroids\n",
    "con=1\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 300]\n",
    "fig = plt.figure()\n",
    "for index1, x in enumerate(vs_num_norm.columns):\n",
    "    for index2, y in enumerate(vs_num_norm.columns):\n",
    "        if x != y:\n",
    "            #print(index2)\n",
    "            #print(index1)\n",
    "            #print(centers[:, index1])\n",
    "            #print(centers[:, index2])\n",
    "            ax = fig.add_subplot((len(vs_num_norm.columns)*(len(vs_num_norm.columns)-1))/3, 3, con)\n",
    "            ax.scatter(vs_num[x], vs_num[y], c=clusters, s=20)\n",
    "            ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"r\")\n",
    "            #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "            ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            con+=1\n",
    "fig.show()\n",
    "print()\n",
    "\n",
    "# Visualization of centers in parallel coordinates\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_basic_statistics(vs_num, clusters, vs_num.columns)\n",
    "plot_basic_statistics(vs_num_norm, clusters, vs_num.columns)\n",
    "\n",
    "\n",
    "# Visualization of centers in radar plot \n",
    "N = len(vs_num.columns)\n",
    "for i in range(0, len(scaled_centers)):\n",
    "    angles = [n / float(N) *2 *pi for n in range(N)]\n",
    "    values = scaled_centers[i].tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    ax = plt.subplot(polar=True)\n",
    "    plt.xticks(angles[:-1], vs_num.columns, color=\"grey\", size=10)\n",
    "    ax.plot(angles, values, linewidth=1, linestyle=\"solid\")\n",
    "    ax.fill(angles, values, \"b\", alpha=0.1)\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "# Visualization with categorical attributes\n",
    "for x in vs_cat.columns:\n",
    "        var_val_xt = pd.crosstab(clusters, vs_cat[x])\n",
    "        var_val_xt.plot(kind='bar', stacked=False, figsize = (20,15), fontsize=(12))\n",
    "        plt.title(x + ' by clusters')\n",
    "        plt.ylabel(x)\n",
    "        plt.xlabel('Clusters')\n",
    "        #plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-crosstab-clustersby\" + x + \".jpg\")\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "# Observing the size of each cluster\n",
    "hist, bins = np.histogram(clusters, bins=range(0, len(set(clusters)) +1))\n",
    "sizes = dict(zip(bins, hist))\n",
    "\n",
    "for id_cluster, size in sizes.items():\n",
    "    print(\"Cluster %d: %d (%.2f)\" % (id_cluster, size, size/len(vs_num_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_vs_num_norm)\n",
    "#print(len(labels))\n",
    "plot_silhouette_score(pca_vs_num_norm, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "profile_kmeans = vs_num.copy()\n",
    "profile_kmeans['cluster'] = clusters\n",
    "profile_kmeans['cluster'] = profile_kmeans['cluster'].map({\n",
    "    0:'4', \n",
    "    1: '3', \n",
    "    2: '2',\n",
    "    3: '1'})\n",
    "\n",
    "sns.pairplot(profile_kmeans, hue='cluster', diag_kind='kde', height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- K = 7 --------------------\n",
    "k_means = cluster.KMeans(n_clusters=7, max_iter=100)   \n",
    "clusters = k_means.fit_predict(pca_vs_num_norm)\n",
    "#k_means = k_means.fit(vs_num_norm)\n",
    "#p_k_means = k_means.predict(vs_num_norm)\n",
    "#labels = k_means.labels_\n",
    "centroids = k_means.cluster_centers_\n",
    "#print(centroids)\n",
    "        \n",
    "print(np.unique(clusters, return_counts=True))\n",
    "\n",
    "\"\"\"for x in pca_vs_num_norm.columns:\n",
    "    for y in pca_vs_num_norm.columns:\n",
    "        for z in pca_vs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                plt.figure(figsize=(20, 15))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=pca_vs_num_norm[x],\n",
    "                    y=pca_vs_num_norm[y],\n",
    "                    z=pca_vs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=labels, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                                scene = dict(\n",
    "                                    xaxis_title= x,\n",
    "                                    yaxis_title= y,\n",
    "                                    zaxis_title= z)\n",
    "                                )\n",
    "                #fig.write_image(dir + \"\\\\Clustering\\\\KMeans\\\\3d-after-kmeans.jpg\")\n",
    "                fig.show()\"\"\"\n",
    "\n",
    "\n",
    "# Visualization of centers in 2D\n",
    "print(centroids.shape)\n",
    "centers = pca.inverse_transform(centroids)\n",
    "print(centers.shape)\n",
    "scaled_centers = centers\n",
    "centers = scaler.inverse_transform(centers)\n",
    "print(centers.shape)\n",
    "#centers = centroids\n",
    "con=1\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 300]\n",
    "fig = plt.figure()\n",
    "for index1, x in enumerate(vs_num_norm.columns):\n",
    "    for index2, y in enumerate(vs_num_norm.columns):\n",
    "        if x != y:\n",
    "            #print(index2)\n",
    "            #print(index1)\n",
    "            #print(centers[:, index1])\n",
    "            #print(centers[:, index2])\n",
    "            ax = fig.add_subplot((len(vs_num_norm.columns)*(len(vs_num_norm.columns)-1))/3, 3, con)\n",
    "            ax.scatter(vs_num[x], vs_num[y], c=clusters, s=20)\n",
    "            ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"r\")\n",
    "            #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "            ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            con+=1\n",
    "fig.show()\n",
    "print()\n",
    "\n",
    "# Visualization of centers in parallel coordinates\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_basic_statistics(vs_num, clusters, vs_num.columns)\n",
    "plot_basic_statistics(vs_num_norm, clusters, vs_num.columns)\n",
    "\n",
    "\n",
    "# Visualization of centers in radar plot \n",
    "N = len(vs_num.columns)\n",
    "for i in range(0, len(scaled_centers)):\n",
    "    angles = [n / float(N) *2 *pi for n in range(N)]\n",
    "    values = scaled_centers[i].tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    ax = plt.subplot(polar=True)\n",
    "    plt.xticks(angles[:-1], vs_num.columns, color=\"grey\", size=10)\n",
    "    ax.plot(angles, values, linewidth=1, linestyle=\"solid\")\n",
    "    ax.fill(angles, values, \"b\", alpha=0.1)\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "# Visualization with categorical attributes\n",
    "for x in vs_cat.columns:\n",
    "        var_val_xt = pd.crosstab(clusters, vs_cat[x])\n",
    "        var_val_xt.plot(kind='bar', stacked=False, figsize = (20,15), fontsize=(12))\n",
    "        plt.title(x + ' by clusters')\n",
    "        plt.ylabel(x)\n",
    "        plt.xlabel('Clusters')\n",
    "        #plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-crosstab-clustersby\" + x + \".jpg\")\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "# Observing the size of each cluster\n",
    "hist, bins = np.histogram(clusters, bins=range(0, len(set(clusters)) +1))\n",
    "sizes = dict(zip(bins, hist))\n",
    "\n",
    "for id_cluster, size in sizes.items():\n",
    "    print(\"Cluster %d: %d (%.2f)\" % (id_cluster, size, size/len(vs_num_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_vs_num_norm)\n",
    "#print(len(labels))\n",
    "plot_silhouette_score(pca_vs_num_norm, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "profile_kmeans = vs_num.copy()\n",
    "profile_kmeans['cluster'] = clusters\n",
    "profile_kmeans['cluster'] = profile_kmeans['cluster'].map({\n",
    "    0:'6', \n",
    "    1: '4', \n",
    "    2: '5',\n",
    "    3: '4',\n",
    "    2: '2',\n",
    "    2: '1',\n",
    "    2: '7'})\n",
    "\n",
    "sns.pairplot(profile_kmeans, hue='cluster', diag_kind='kde', height=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for the vendor & brand dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = []\n",
    "SIL = []\n",
    "SEP = []\n",
    "CAL = []\n",
    "\n",
    "for k in range(2, 20):        \n",
    "    k_means = cluster.KMeans(n_clusters=k, max_iter=100)   # , random_state=1\n",
    "    clusters = k_means.fit_predict(pca_bvs_num_norm)\n",
    "    #k_means = k_means.fit(pca_vs_num_norm)\n",
    "    #p_k_means = k_means.predict(pca_vs_num_norm)\n",
    "    #labels = k_means.labels_\n",
    "    centroids = k_means.cluster_centers_\n",
    "\n",
    "    SSE.append(k_means.inertia_)\n",
    "    SIL.append(silhouette_score(pca_bvs_num_norm, clusters, metric = 'euclidean'))\n",
    "    print(\"For n_clusters =\", k,\n",
    "          \"The average silhouette_score is :\", silhouette_score(pca_bvs_num_norm, clusters, metric = 'euclidean'))    \n",
    "    SEP.append(davies_bouldin_score(pca_bvs_num_norm, clusters))\n",
    "    CAL.append(calinski_harabasz_score(pca_bvs_num_norm, clusters))\n",
    "    \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 15]\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "fig.suptitle('Clustring Evaluation')\n",
    "\n",
    "    \n",
    "ax1.plot(range(2, len(SSE)+2), SSE,'o-')\n",
    "ax1.set_title(\"SSE\")\n",
    "ax1.set_xlabel('Number of Clusters')\n",
    "ax1.set_ylabel('SSE')\n",
    "ax1.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "for x,y in zip(range(2, len(SSE)+2), SSE):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax1.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "#ax1.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\sse.jpg\")\n",
    "#ax1.show()\n",
    "\n",
    "ax2.plot(range(2, len(SIL)+2), SIL,'o-')\n",
    "ax2.set_title(\"Silhouette Score\")\n",
    "ax2.set_xlabel('Number of Clusters')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "#ax2.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\silhouette.jpg\")\n",
    "#ax2.show()\n",
    "for x,y in zip(range(2, len(SIL)+2), SIL):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax2.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "ax3.plot(range(2, len(SEP)+2), SEP,'o-')\n",
    "ax3.set_title(\"Davies Bouldin Index\")\n",
    "ax3.set_xlabel('Number of Clusters')\n",
    "ax3.set_ylabel('Davies Bouldin Index')\n",
    "ax3.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "#ax3.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\davies-bouldin.jpg\")\n",
    "#ax3.show()\n",
    "for x,y in zip(range(2, len(SEP)+2), SEP):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax3.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "ax4.plot(range(2, len(CAL)+2), CAL,'o-')\n",
    "ax4.set_title(\"Calinski-Harabasz Index\")\n",
    "ax4.set_xlabel('Number of Clusters')\n",
    "ax4.set_ylabel('Calinski-Harabasz Index')\n",
    "ax4.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "#ax4.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\calinski.jpg\")\n",
    "#ax4.show()\n",
    "for x,y in zip(range(2, len(CAL)+2), CAL):\n",
    "\n",
    "    label = \"( \"+\"{:.0f}\".format(x)+\" , \"+\"{:.2f}\".format(y)+\")\"\n",
    "\n",
    "    ax4.annotate(label, # this is the text\n",
    "                 (x,y), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [30, 10]\n",
    "plt.title(\"Hierarchical Clustering of the Customers\")\n",
    "plt.axhline(y=80, ls='--', c='red')\n",
    "dend = dendrogram(linkage(pca_bvs_num_norm, method='ward'), leaf_rotation=90, leaf_font_size = 8,show_contracted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centroids analysis with AffinityPropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "affinity = AffinityPropagation()\n",
    "clustering = affinity.fit_predict(pca_bvs_num_norm)\n",
    "#k_means = k_means.fit_predict(vs_num_norm)\n",
    "#p_k_means = k_means.predict(vs_num_norm)\n",
    "#labels = clustering.labels_\n",
    "centroids = affinity.cluster_centers_\n",
    "print('labels: '+str(clustering)) \n",
    "print('labels len: '+str(len(clustering))) \n",
    "print(np.unique(clustering, return_counts=True))\n",
    "\n",
    "\n",
    "# Visualization of centers in 2D\n",
    "centers = pca_bvs.inverse_transform(centroids)\n",
    "centers = scaler_bvs.inverse_transform(centers)\n",
    "con=1\n",
    "#plt.rcParams[\"figure.figsize\"] = [60, 20]\n",
    "# fig = plt.figure()\n",
    "# print(labels)\n",
    "# color_labels=[]\n",
    "# for x in labels:\n",
    "#     if x==0:\n",
    "#         color_labels.append('red')\n",
    "#     if x==1:\n",
    "#         color_labels.append('blue')\n",
    "#     if x==2:\n",
    "#         color_labels.append('green')\n",
    "#     if x==3:\n",
    "#         color_labels.append('black')\n",
    "#     if x==4:\n",
    "#         color_labels.append('brown')\n",
    "#     if x==5:\n",
    "#         color_labels.append('purple')\n",
    "#     if x==6:\n",
    "#         color_labels.append('yellow')        \n",
    "#     if x==7:\n",
    "#         color_labels.append('orange')\n",
    "#     if x==8:\n",
    "#         color_labels.append('gray')        \n",
    "# for index1, x in enumerate(vs_num_norm.columns):\n",
    "#     for index2, y in enumerate(vs_num_norm.columns):\n",
    "#         if x != y:\n",
    "#             ax = fig.add_subplot(15, 2, con)\n",
    "#             ax.scatter(vs_num[x], vs_num[y], c=color_labels, s=20)\n",
    "#             ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"green\")\n",
    "#             #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "#             ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "#             con+=1\n",
    "# fig.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "cluster_centers_indices = affinity.cluster_centers_indices_\n",
    "\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "print(n_clusters_)\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "\n",
    "#####https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "# for k, col in zip(range(n_clusters_), colors):\n",
    "#     class_members = labels == k\n",
    "#     cluster_center = tmp[cluster_centers_indices[k]]\n",
    "#     plt.plot(tmp[class_members, 0], tmp[class_members, 1], col + '.')\n",
    "#     plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "#              markeredgecolor='k', markersize=14)\n",
    "#     for x in tmp[class_members]:\n",
    "#         plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "# plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "con=1\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 350]\n",
    "fig = plt.figure()\n",
    "for index1, x in enumerate(bvs_num_norm.columns):\n",
    "    for index2, y in enumerate(bvs_num_norm.columns):\n",
    "        if x != y:\n",
    "           \n",
    "\n",
    "            tmp=bvs_num_norm[[x,y]]\n",
    "            \n",
    "            tmp=tmp.to_numpy()\n",
    "            ax = fig.add_subplot((len(bvs_num_norm.columns)*(len(bvs_num_norm.columns)-1))/3, 3, con)\n",
    "            for k, col in zip(range(n_clusters_), colors):\n",
    "                class_members = clustering == k\n",
    "                cluster_center = tmp[cluster_centers_indices[k]]\n",
    "                \n",
    "                ax.plot(tmp[class_members, 0], tmp[class_members, 1], col + '.')\n",
    "                ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "                         markeredgecolor='k', markersize=14)\n",
    "                for z in tmp[class_members]:\n",
    "                    ax.plot([cluster_center[0], z[0]], [cluster_center[1], z[1]], col)\n",
    "#             ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            ax.set_title(\"Estimated number of clusters: \" +str(n_clusters_)+\" visualization on \" + x + \" and \" + y)\n",
    "            con+=1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster indication:\n",
    "SSE= 3...6\n",
    "DB= 4...9\n",
    "CAL= 4...8\n",
    "SIL= 3...6, 8-9\n",
    "\n",
    "Dendrogram = 2-3\n",
    "\n",
    "Affinity= >= 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALORI DI CLUSTERING = 4-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- K = 4 --------------------\n",
    "k_means = cluster.KMeans(n_clusters=4, max_iter=100)   \n",
    "clusters = k_means.fit_predict(pca_bvs_num_norm)\n",
    "#k_means = k_means.fit(vs_num_norm)\n",
    "#p_k_means = k_means.predict(vs_num_norm)\n",
    "#labels = k_means.labels_\n",
    "centroids = k_means.cluster_centers_\n",
    "#print(centroids)\n",
    "        \n",
    "print(np.unique(clusters, return_counts=True))\n",
    "\n",
    "\"\"\"for x in pca_vs_num_norm.columns:\n",
    "    for y in pca_vs_num_norm.columns:\n",
    "        for z in pca_vs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                plt.figure(figsize=(20, 15))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=pca_vs_num_norm[x],\n",
    "                    y=pca_vs_num_norm[y],\n",
    "                    z=pca_vs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=labels, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                                scene = dict(\n",
    "                                    xaxis_title= x,\n",
    "                                    yaxis_title= y,\n",
    "                                    zaxis_title= z)\n",
    "                                )\n",
    "                #fig.write_image(dir + \"\\\\Clustering\\\\KMeans\\\\3d-after-kmeans.jpg\")\n",
    "                fig.show()\"\"\"\n",
    "\n",
    "\n",
    "# Visualization of centers in 2D\n",
    "print(centroids.shape)\n",
    "centers = pca_bvs.inverse_transform(centroids)\n",
    "print(centers.shape)\n",
    "scaled_centers = centers\n",
    "centers = scaler_bvs.inverse_transform(centers)\n",
    "print(centers.shape)\n",
    "#centers = centroids\n",
    "con=1\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 300]\n",
    "fig = plt.figure()\n",
    "for index1, x in enumerate(bvs_num_norm.columns):\n",
    "    for index2, y in enumerate(bvs_num_norm.columns):\n",
    "        if x != y:\n",
    "            #print(index2)\n",
    "            #print(index1)\n",
    "            #print(centers[:, index1])\n",
    "            #print(centers[:, index2])\n",
    "            ax = fig.add_subplot((len(bvs_num_norm.columns)*(len(bvs_num_norm.columns)-1))/3, 3, con)\n",
    "            ax.scatter(bvs_num[x], bvs_num[y], c=clusters, s=20)\n",
    "            ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"r\")\n",
    "            #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "            ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            con+=1\n",
    "fig.show()\n",
    "\n",
    "# Visualization of centers in parallel coordinates\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_basic_statistics(bvs_num, clusters, bvs_num.columns)\n",
    "plot_basic_statistics(bvs_num_norm, clusters, bvs_num.columns)\n",
    "\n",
    "\n",
    "# Visualization of centers in radar plot \n",
    "N = len(bvs_num.columns)\n",
    "for i in range(0, len(scaled_centers)):\n",
    "    angles = [n / float(N) *2 *pi for n in range(N)]\n",
    "    values = scaled_centers[i].tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    ax = plt.subplot(polar=True)\n",
    "    plt.xticks(angles[:-1], vs_num.columns, color=\"grey\", size=10)\n",
    "    ax.plot(angles, values, linewidth=1, linestyle=\"solid\")\n",
    "    ax.fill(angles, values, \"b\", alpha=0.1)\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "# Visualization with categorical attributes\n",
    "for x in bvs_cat.columns:\n",
    "        var_val_xt = pd.crosstab(clusters, vs_cat[x])\n",
    "        var_val_xt.plot(kind='bar', stacked=False, figsize = (20,15), fontsize=(12))\n",
    "        plt.title(x + ' by clusters')\n",
    "        plt.ylabel(x)\n",
    "        plt.xlabel('Clusters')\n",
    "        #plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-crosstab-clustersby\" + x + \".jpg\")\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "# Observing the size of each cluster\n",
    "hist, bins = np.histogram(clusters, bins=range(0, len(set(clusters)) +1))\n",
    "sizes = dict(zip(bins, hist))\n",
    "\n",
    "for id_cluster, size in sizes.items():\n",
    "    print(\"Cluster %d: %d (%.2f)\" % (id_cluster, size, size/len(bvs_num_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_vs_num_norm)\n",
    "#print(len(labels))\n",
    "plot_silhouette_score(pca_bvs_num_norm, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "profile_kmeans = bvs_num.copy()\n",
    "profile_kmeans['cluster'] = clusters\n",
    "profile_kmeans['cluster'] = profile_kmeans['cluster'].map({\n",
    "    0:'4', \n",
    "    1: '2', \n",
    "    2: '3',\n",
    "    3: '1'})\n",
    "\n",
    "sns.pairplot(profile_kmeans, hue='cluster', diag_kind='kde', height=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- K = 7 --------------------\n",
    "k_means = cluster.KMeans(n_clusters=7, max_iter=100)   \n",
    "clusters = k_means.fit_predict(pca_bvs_num_norm)\n",
    "#k_means = k_means.fit(vs_num_norm)\n",
    "#p_k_means = k_means.predict(vs_num_norm)\n",
    "#labels = k_means.labels_\n",
    "centroids = k_means.cluster_centers_\n",
    "#print(centroids)\n",
    "        \n",
    "print(np.unique(clusters, return_counts=True))\n",
    "\n",
    "\"\"\"for x in pca_vs_num_norm.columns:\n",
    "    for y in pca_vs_num_norm.columns:\n",
    "        for z in pca_vs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                plt.figure(figsize=(20, 15))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=pca_vs_num_norm[x],\n",
    "                    y=pca_vs_num_norm[y],\n",
    "                    z=pca_vs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=labels, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                                scene = dict(\n",
    "                                    xaxis_title= x,\n",
    "                                    yaxis_title= y,\n",
    "                                    zaxis_title= z)\n",
    "                                )\n",
    "                #fig.write_image(dir + \"\\\\Clustering\\\\KMeans\\\\3d-after-kmeans.jpg\")\n",
    "                fig.show()\"\"\"\n",
    "\n",
    "\n",
    "# Visualization of centers in 2D\n",
    "print(centroids.shape)\n",
    "centers = pca_bvs.inverse_transform(centroids)\n",
    "print(centers.shape)\n",
    "scaled_centers = centers\n",
    "centers = scaler_bvs.inverse_transform(centers)\n",
    "print(centers.shape)\n",
    "#centers = centroids\n",
    "con=1\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 300]\n",
    "fig = plt.figure()\n",
    "for index1, x in enumerate(bvs_num_norm.columns):\n",
    "    for index2, y in enumerate(bvs_num_norm.columns):\n",
    "        if x != y:\n",
    "            #print(index2)\n",
    "            #print(index1)\n",
    "            #print(centers[:, index1])\n",
    "            #print(centers[:, index2])\n",
    "            ax = fig.add_subplot((len(bvs_num_norm.columns)*(len(bvs_num_norm.columns)-1))/3, 3, con)\n",
    "            ax.scatter(bvs_num[x], bvs_num[y], c=clusters, s=20)\n",
    "            ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"r\")\n",
    "            #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "            ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            con+=1\n",
    "fig.show()\n",
    "\n",
    "# Visualization of centers in parallel coordinates\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_basic_statistics(bvs_num, clusters, bvs_num.columns)\n",
    "plot_basic_statistics(bvs_num_norm, clusters, bvs_num.columns)\n",
    "\n",
    "\n",
    "# Visualization of centers in radar plot \n",
    "N = len(bvs_num.columns)\n",
    "for i in range(0, len(scaled_centers)):\n",
    "    angles = [n / float(N) *2 *pi for n in range(N)]\n",
    "    values = scaled_centers[i].tolist()\n",
    "    values += values[:1]\n",
    "    angles += angles[:1]\n",
    "    ax = plt.subplot(polar=True)\n",
    "    plt.xticks(angles[:-1], vs_num.columns, color=\"grey\", size=10)\n",
    "    ax.plot(angles, values, linewidth=1, linestyle=\"solid\")\n",
    "    ax.fill(angles, values, \"b\", alpha=0.1)\n",
    "plt.show()\n",
    "print()\n",
    "\n",
    "# Visualization with categorical attributes\n",
    "for x in bvs_cat.columns:\n",
    "        var_val_xt = pd.crosstab(clusters, vs_cat[x])\n",
    "        var_val_xt.plot(kind='bar', stacked=False, figsize = (20,15), fontsize=(12))\n",
    "        plt.title(x + ' by clusters')\n",
    "        plt.ylabel(x)\n",
    "        plt.xlabel('Clusters')\n",
    "        #plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-crosstab-clustersby\" + x + \".jpg\")\n",
    "        plt.show()\n",
    "        print()\n",
    "\n",
    "# Observing the size of each cluster\n",
    "hist, bins = np.histogram(clusters, bins=range(0, len(set(clusters)) +1))\n",
    "sizes = dict(zip(bins, hist))\n",
    "\n",
    "for id_cluster, size in sizes.items():\n",
    "    print(\"Cluster %d: %d (%.2f)\" % (id_cluster, size, size/len(bvs_num_norm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_vs_num_norm)\n",
    "#print(len(labels))\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_silhouette_score(pca_bvs_num_norm, clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "profile_kmeans = bvs_num.copy()\n",
    "profile_kmeans['cluster'] = clusters\n",
    "profile_kmeans['cluster'] = profile_kmeans['cluster'].map({\n",
    "    0:'7', \n",
    "    1: '6', \n",
    "    2: '4',\n",
    "    3: '1',\n",
    "    4: '3',\n",
    "    5: '5',\n",
    "    6: '2'})\n",
    "\n",
    "sns.pairplot(profile_kmeans, hue='cluster', diag_kind='kde', height=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison and evaluation of the different clustering via internal metrics\n",
    "\n",
    "Regarding the Sum of Squared Errors (SSE): - a decrease in the SSE value proportional to\n",
    "the number of clusters is an expected behavior, therefore opting directly for the clustering with the\n",
    "lowest sum of squared distances may not be a worthwhile decision.\n",
    "Regarding the Davies Bouldini Index: - a lower Davies-Bouldin index relates to a model with\n",
    "better separation between the clusters and, in this regard, the clustering with k equals to 2 seems\n",
    "to present the best separation among its clusters.\n",
    "Regarding the Silhouette Score: - a higher value for the Silhouette Coefficient relates to a model\n",
    "with better defined clusters, in this regard, the clustering with k equals to 2 presents the best\n",
    "score.\n",
    "Regarding the Calinski-Harabasz Index: - similarly to the Silhouette Coefficient, a higher value\n",
    "for the Calinski-Harabasz score relates to a model with better defined clusters, in this regard, the\n",
    "clustering with k equals to 2 seems to present the best defined clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of (eventually) anomalous cluster [page 7 Clustering-altro.pdf]\n",
    "- non abbiamo cluster con un unico valore, ma devo fare ulteriori valutazioni sulle visualizzazioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers\n",
    "Task_2 pag. 13, ma non li elimina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison results with external/internal indexes:\n",
    "\n",
    "- (internal) evaluation metrics are SSE, SIL, DB, CAL\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by density-based clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best values for eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=x)\n",
    "neighbors = knn.fit(pca_bvs_num_norm)\n",
    "distances, indices = neighbors.kneighbors(pca_bvs_num_norm)\n",
    "\n",
    "distances = np.sort(distances, axis=0)[:, 1]\n",
    "plt.plot(distances)\n",
    "\n",
    "\n",
    "# [page 30 Task_2.pdf]\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "# Density Plot and Histogram\n",
    "sns.distplot(distances, kde=True)\n",
    "plt.ylabel('Neighbors')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.tick_params(axis='both', which='major')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.1-0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----knee method--------\n",
    "dist = pdist(bvs_num_norm, 'euclidean')\n",
    "dist = squareform(dist)\n",
    "\n",
    "kth_distances = {k:[] for k in range(2, 20 + 1)}\n",
    "for d in dist:\n",
    "    index_kth_distance = np.argsort(d)  #[round(k)]\n",
    "    for i in range(2, 20+1):\n",
    "        kth_distances[i].append(d[index_kth_distance[i]])\n",
    "\n",
    "for k in kth_distances.keys():\n",
    "    plt.plot(range(0, len(kth_distances[k])), sorted(kth_distances[k]))\n",
    "plt.ylabel('Dist eps', fontsize=18)\n",
    "plt.xlabel('Sorted distances', fontsize=18)\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\DBSCAN\\\\eps.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.25...0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------  DBSCAN-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "eps = []\n",
    "p=0.1\n",
    "for i in range(0, 10):    #0.35-0.75\n",
    "    eps.append(p)\n",
    "    p+=0.01\n",
    "    \n",
    "for i in eps:\n",
    "    for n in range (4, vs_num.shape[1]*2):     #68rows -> ln= 4\n",
    "        dbscan = DBSCAN(eps=i, min_samples=n)\n",
    "        clusters = dbscan.fit_predict(pca_bvs_num_norm)\n",
    "        core_samples_mask = np.zeros_like(clusters, dtype=bool)\n",
    "        core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "        #remove -1 for outliers\n",
    "        #clusters = set([label for label in clusters if label >= 0])\n",
    "        if len(np.unique(clusters, return_counts=True)[0]) <= 1:\n",
    "            continue\n",
    "        #print(np.unique(clusters, return_counts=True))\n",
    "        \n",
    "        res.append({\n",
    "            'label': len(np.unique(clusters, return_counts=True)[0])-1,\n",
    "            'sil': silhouette_score(bvs_num_norm, clusters),\n",
    "            'ms': n,\n",
    "            'eps': i\n",
    "        })\n",
    "\n",
    "\n",
    "print(*res, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'aumentare dei min_samples(ms) il valore dei label e del silhouette score diminuisce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=i, min_samples=n)\n",
    "clusters = dbscan.fit_predict(pca_bvs_num_norm)\n",
    "core_samples_mask = np.zeros_like(clusters, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "print(\"Number of cluster from DBSCAN is: \", len(np.unique(clusters, return_counts=True)[0])-1)\n",
    "print(np.unique(clusters, return_counts=True))\n",
    "\n",
    "\n",
    "con=1\n",
    "#plt.rcParams[\"figure.figsize\"] = [30, 100]\n",
    "fig = plt.figure()\n",
    "for x in bvs_num_norm.columns:\n",
    "    for y in bvs_num_norm.columns:\n",
    "        if x != y:\n",
    "            ax = fig.add_subplot(15, 2, con)\n",
    "            ax.scatter(bvs_num[x], bvs_num[y], c=clusters, s=20)\n",
    "            #ax.scatter(centers[:, index1], centers[:, index2], s=200, marker=\"*\", c=\"r\")\n",
    "            #ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "            #ax.set_title(\"CLuster (color) visualization on \" + x + \" and \" + y )\n",
    "            con+=1\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\"\"\"for x in bvs_num_norm.columns:\n",
    "    for y in bvs_num_norm.columns:\n",
    "        for z in bvs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                plt.figure(figsize=(20, 15))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=bvs_num_norm[x],\n",
    "                    y=bvs_num_norm[y],\n",
    "                    z=bvs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=clusters, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                                scene = dict(\n",
    "                                    xaxis_title= x,\n",
    "                                    yaxis_title= y,\n",
    "                                    zaxis_title= z)\n",
    "                                )\n",
    "                #fig.write_image('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\DBSCAN\\\\3d-after-dbscan.jpg')\n",
    "                fig.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "profile_dbscan = vs_num.copy()\n",
    "profile_dbscan['cluster'] = clusters\n",
    "profile_dbscan['cluster'] = profile_dbscan['cluster'].map({\n",
    "    0:'4', \n",
    "    1: '3', \n",
    "    2: '2',\n",
    "    3: '1'})\n",
    "\n",
    "sns.pairplot(profile_dbscan, hue='cluster', diag_kind='kde', height=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----dendrogram----\n",
    "linkages = ['ward', 'complete', 'average', 'single']\n",
    "metrics = ['euclidean', 'manhattan']\n",
    "\n",
    "#--------------- SISTEMARE COLORE PLOT\n",
    "for i in linkages:\n",
    "    plt.title('Link: ' + i)\n",
    "    dist = pdist(pca_bvs_num_norm, metric='euclidean')\n",
    "    link = linkage(dist, method=i, metric='euclidean')\n",
    "    res1 = dendrogram(link, color_threshold=0.7, truncate_mode='lastp', orientation='top',\n",
    "                      distance_sort='descending', show_leaf_counts=True)\n",
    "    #plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\dendrogram-linkages-' + i + '.jpg')\n",
    "    plt.show()\n",
    "\n",
    "res2 = []\n",
    "for k in range(3, 14):\n",
    "    for l in linkages:\n",
    "        for metr in metrics:\n",
    "            if l == 'ward' and metr != 'euclidean':\n",
    "                continue\n",
    "            hiera = AgglomerativeClustering(n_clusters=k, affinity=metr, linkage=l)\n",
    "            clusters = hiera.fit_predict(pca_bvs_num_norm)\n",
    "            hist, bins = np.histogram(clusters, bins=range(0, len(set(clusters)) + 1))\n",
    "            res2.append({\n",
    "                #'hiera': hiera,\n",
    "                #'res': clusters,\n",
    "                'labels': dict(zip(bins, hist)),\n",
    "                'n_clusters': k,\n",
    "                'sil': silhouette_score(pca_bvs_num_norm, clusters, metric = metr),\n",
    "                'link': l,\n",
    "                'metric': metr\n",
    "            })\n",
    "\n",
    "print(*res2, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(3, 14)\n",
    "fig, ax = plt.subplots()\n",
    "y1 = [a for a in res2 if a['link'] == 'single' and a['metric'] == 'euclidean']\n",
    "y2 = [a for a in res2 if a['link'] == 'ward' and a['metric'] == 'euclidean']\n",
    "y3 = [a for a in res2 if a['link'] == 'complete' and a['metric'] == 'euclidean']\n",
    "y4 = [a for a in res2 if a['link'] == 'average' and a['metric'] == 'euclidean']\n",
    "ys = [y1, y2, y3, y4]\n",
    "for e in ys:\n",
    "    l = e[0]['link']\n",
    "    ax.plot(x, [s['sil'] for s in e], label=l)\n",
    "ax.set_title('Metric: Euclidean')\n",
    "plt.legend(fontsize=10)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-cluster-plot.jpg')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y1 = [a for a in res2 if a['link'] == 'single' and a['metric'] == 'manhattan']                               \n",
    "y3 = [a for a in res2 if a['link'] == 'complete' and a['metric'] == 'manhattan']\n",
    "y4 = [a for a in res2 if a['link'] == 'average' and a['metric'] == 'manhattan']\n",
    "ys = [y1, y3, y4]\n",
    "for e in ys:\n",
    "    l = e[0]['link']\n",
    "    ax.plot(x, [s['sil'] for s in e], label=l)\n",
    "ax.set_title('Metric: Manhattan')\n",
    "plt.legend(fontsize=10)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-manhattan-plot.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean:\n",
    "- Single: decreasing\n",
    "- Ward: 4...6\n",
    "- Complete: 4-5\n",
    "- Average: 6-7-9\n",
    "\n",
    "Manhattan:\n",
    "- Single: decreasing\n",
    "- Complete: 4-5-7\n",
    "- Average: 4-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the euclidean metric with average linkage to obtain 7/9 clusters and the Manhattan\n",
    "distance with average linkage to obtain 5 clusters. \n",
    "\"...The number of clusters was chosen in order to\n",
    "maximize the silhouette score while still retaining a decent amount of clusters...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pdist(pca_bvs_num_norm, metric='euclidean')\n",
    "link = linkage(dist, method=\"average\", metric='euclidean')\n",
    "res = dendrogram(link, color_threshold=0.7, truncate_mode='lastp', orientation='top',\n",
    "                    distance_sort='descending', show_leaf_counts=True)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\dendrogram-linkages-' + i + '.jpg')\n",
    "plt.show()\n",
    "\n",
    "dist = pdist(pca_bvs_num_norm, metric='cityblock')\n",
    "link = linkage(dist, method=\"average\", metric='manhattan')\n",
    "res = dendrogram(link, color_threshold=0.7, truncate_mode='lastp', orientation='top',\n",
    "                    distance_sort='descending', show_leaf_counts=True)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\dendrogram-linkages-' + i + '.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean_avg_agl = AgglomerativeClustering(n_clusters=7, affinity=\"euclidean\", linkage=\"average\")\n",
    "clusters = euclidean_avg_agl.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "euclidean_avg_agl = AgglomerativeClustering(n_clusters=9, affinity=\"euclidean\", linkage=\"average\")\n",
    "clusters = euclidean_avg_agl.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "euclidean_avg_agl = AgglomerativeClustering(n_clusters=5, affinity=\"euclidean\", linkage=\"complete\")\n",
    "clusters = euclidean_avg_agl.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "euclidean_avg_agl = AgglomerativeClustering(n_clusters=6, affinity=\"euclidean\", linkage=\"ward\")\n",
    "clusters = euclidean_avg_agl.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "\n",
    "\"\"\"profile_hiera = bvs_num.copy()\n",
    "profile_hiera['cluster'] = clusters\n",
    "profile_hiera['cluster'] = profile_hiera['cluster'].map({\n",
    "    0:'4', \n",
    "    1: '3', \n",
    "    2: '2',\n",
    "    3: '1'})\n",
    "\n",
    "sns.pairplot(profile_hiera, hue='cluster', diag_kind='kde', height=1.5)\n",
    "\n",
    "for i in bvs_num_norm.columns:\n",
    "    for z in bvs_num_norm.columns:\n",
    "        if i == z:\n",
    "            continue\n",
    "        plt.figure(figsize=(18, 10))\n",
    "        plt.scatter(bvs_num_norm[i], bvs_num_norm[z], c=clusters, cmap='rainbow')\n",
    "        plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "        plt.xlabel(i)\n",
    "        plt.ylabel(z)\n",
    "        #plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-scatter-'+i +'-' +z + '.jpg')\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "\"\"\"for x in bvs_num_norm.columns:\n",
    "    for y in bvs_num_norm.columns:\n",
    "        for z in bvs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                fig = plt.figure(figsize=(16,12))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=bvs_num_norm[x],\n",
    "                    y=bvs_num_norm[y],\n",
    "                    z=bvs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        color=clusters, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                            scene = dict(\n",
    "                                xaxis_title= x,\n",
    "                                yaxis_title= y,\n",
    "                                zaxis_title= z)\n",
    "                            )\n",
    "                #fig.write_image('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\3d-after-hiera.jpg')\n",
    "                fig.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_avg_agl = AgglomerativeClustering(n_clusters=5, affinity=\"manhattan\", linkage=\"average\")\n",
    "clusters = manhattan_avg_agl.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "manhattan_avg_agl = AgglomerativeClustering(n_clusters=4, affinity=\"manhattan\", linkage=\"average\")\n",
    "clusters = manhattan_avg_agl.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "tmp = pd.DataFrame(pd.Series(clusters).value_counts(), columns=['Clusters']).T.sort_index(axis=1)\n",
    "print(tmp)\n",
    "\n",
    "\"\"\"profile_hiera = bvs_num.copy()\n",
    "profile_hiera['cluster'] = clusters\n",
    "profile_hiera['cluster'] = profile_hiera['cluster'].map({\n",
    "    0:'4', \n",
    "    1: '3', \n",
    "    2: '2',\n",
    "    3: '1'})\n",
    "\n",
    "sns.pairplot(profile_hiera, hue='cluster', diag_kind='kde', height=1.5)\n",
    "\n",
    "for i in bvs_num_norm.columns:\n",
    "    for z in bvs_num_norm.columns:\n",
    "        if i == z:\n",
    "            continue\n",
    "        plt.figure(figsize=(18, 10))\n",
    "        plt.scatter(bvs_num_norm[i], bvs_num_norm[z], c=clusters, cmap='rainbow')\n",
    "        plt.tick_params(axis='both', which='major', labelsize=22)\n",
    "        plt.xlabel(i)\n",
    "        plt.ylabel(z)\n",
    "        #plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-scatter-'+i +'-' +z + '.jpg')\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "\"\"\"for x in bvs_num_norm.columns:\n",
    "    for y in bvs_num_norm.columns:\n",
    "        for z in bvs_num_norm.columns:\n",
    "            if (x != y) & (x != z) & (y != z):\n",
    "                fig = plt.figure(figsize=(16,12))\n",
    "                fig = go.Figure(data=[go.Scatter3d(\n",
    "                    x=bvs_num_norm[x],\n",
    "                    y=bvs_num_norm[y],\n",
    "                    z=bvs_num_norm[z],\n",
    "                    mode='markers',\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        color=clusters, \n",
    "                        opacity=1\n",
    "                    )\n",
    "                )])\n",
    "                # tight layout\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n",
    "                            scene = dict(\n",
    "                                xaxis_title= x,\n",
    "                                yaxis_title= y,\n",
    "                                zaxis_title= z)\n",
    "                            )\n",
    "                #fig.write_image('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\3d-after-hiera.jpg')\n",
    "                fig.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OPTIONAL) Alternative clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the library: https://github.com/annoviko/pyclustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in range(3, 14):\n",
    "    for l in linkages:\n",
    "        for metr in metrics:\n",
    "            if l == 'ward' and metr != 'euclidean':\n",
    "                continue\n",
    "            agglo = AgglomerativeClustering(n_clusters=k, linkage=l, affinity=metr, connectivity=None)\n",
    "            clusters = agglo.fit_predict(pca_bvs_num_norm)\n",
    "            hist, bins = np.histogram(clusters, bins=range(0, len(set(clusters)) + 1))\n",
    "            res.append({\n",
    "                #'hiera': hiera,\n",
    "                #'res': clusters,\n",
    "                'labels': dict(zip(bins, hist)),\n",
    "                'n_clusters': k,\n",
    "                'sil': silhouette_score(pca_bvs_num_norm, clusters, metric = metr),\n",
    "                'link': l,\n",
    "                'metric': metr\n",
    "            })\n",
    "\n",
    "print(*res, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(3, 14)\n",
    "fig, ax = plt.subplots()\n",
    "y1 = [a for a in res if a['link'] == 'single' and a['metric'] == 'euclidean']\n",
    "y2 = [a for a in res if a['link'] == 'ward' and a['metric'] == 'euclidean']\n",
    "y3 = [a for a in res if a['link'] == 'complete' and a['metric'] == 'euclidean']\n",
    "y4 = [a for a in res if a['link'] == 'average' and a['metric'] == 'euclidean']\n",
    "ys = [y1, y2, y3, y4]\n",
    "for e in ys:\n",
    "    l = e[0]['link']\n",
    "    ax.plot(x, [s['sil'] for s in e], label=l)\n",
    "ax.set_title('Metric: Euclidean')\n",
    "plt.legend(fontsize=10)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-cluster-plot.jpg')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y1 = [a for a in res if a['link'] == 'single' and a['metric'] == 'manhattan']                               \n",
    "y3 = [a for a in res if a['link'] == 'complete' and a['metric'] == 'manhattan']\n",
    "y4 = [a for a in res if a['link'] == 'average' and a['metric'] == 'manhattan']\n",
    "ys = [y1, y3, y4]\n",
    "for e in ys:\n",
    "    l = e[0]['link']\n",
    "    ax.plot(x, [s['sil'] for s in e], label=l)\n",
    "ax.set_title('Metric: Manhattan')\n",
    "plt.legend(fontsize=10)\n",
    "#plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-manhattan-plot.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo = AgglomerativeClustering(n_clusters=5, linkage=\"ward\", affinity=\"euclidean\", connectivity=None)\n",
    "clusters = agglo.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "# Visualization of centers in parallel coordinates\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_basic_statistics(bvs_num, clusters, bvs_num.columns)\n",
    "plot_basic_statistics(bvs_num_norm, clusters, bvs_num.columns)\n",
    "\n",
    "agglo = AgglomerativeClustering(n_clusters=6, linkage=\"average\", affinity=\"manhattan\", connectivity=None)\n",
    "clusters = agglo.fit_predict(pca_bvs_num_norm)\n",
    "\n",
    "# Visualization of centers in parallel coordinates\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "plot_basic_statistics(bvs_num, clusters, bvs_num.columns)\n",
    "plot_basic_statistics(bvs_num_norm, clusters, bvs_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(3,9+1):\n",
    "    birch = Birch(n_clusters=x)\n",
    "    clusters = birch.fit_predict(pca_bvs_num_norm)\n",
    "    print(\"Number of cluster considered\" + x)\n",
    "    plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "    plot_basic_statistics(bvs_num, clusters, bvs_num.columns)\n",
    "    plot_basic_statistics(bvs_num_norm, clusters, bvs_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################# END ######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FUTURE CONSIDERATIONS\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe0fd05f9ecc1bba0c525d982046f76f8af1c67c8117e2e90af1b7cd44f1ddc6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
