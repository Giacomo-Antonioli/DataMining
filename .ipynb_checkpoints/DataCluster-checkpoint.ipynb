{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings & Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--0.0---------------------  SETTINGS -----------------------------------------\n",
    "\"\"\"\n",
    "Data Settings & Importing Libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import sys\n",
    "import os\n",
    "#!conda install --yes --prefix {sys.prefix} plotly\n",
    "#import plotly.graph_objects as go\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#import plotly.io as pio\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import mode\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from sklearn.decomposition import PCA\n",
    "from pandas.plotting import scatter_matrix\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 10]\n",
    "plt.style.use('Solarize_Light2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dropped = pd.read_csv(dir + './df_dropped.csv')\r\n",
    "vs_num = pd.read_csv(dir + '\\\\Data\\\\vs_num.csv', index_col=0)\r\n",
    "vs_cat = pd.read_csv(dir + '\\\\Data\\\\vs_cat.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.info()\r\n",
    "print(\"---------------------------------------------------------------\")\r\n",
    "print(df_dropped.describe(include='all'))\r\n",
    "print(\"---------------------------------------------------------------\")\r\n",
    "print(df_dropped.head())\r\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_num.info()\r\n",
    "print(\"---------------------------------------------------------------\")\r\n",
    "print(vs_num.describe(include='all'))\r\n",
    "print(\"---------------------------------------------------------------\")\r\n",
    "print(vs_num.head())\r\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_cat.info()\r\n",
    "print(\"---------------------------------------------------------------\")\r\n",
    "print(vs_cat.describe(include='all'))\r\n",
    "print(\"---------------------------------------------------------------\")\r\n",
    "print(vs_cat.head())\r\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------- df_dropped dataset\r\n",
    "# conversion \r\n",
    "df_dropped['id_ram'] = df_dropped['id_ram'].astype(object)\r\n",
    "df_dropped['memory_dim'] = df_dropped['memory_dim'].astype(object)\r\n",
    "df_dropped['clock'] = df_dropped['clock'].astype(object)\r\n",
    "df_dropped['time_code'] = pd.to_datetime(df_dropped['time_code'], format='%Y-%m-%d')\r\n",
    "\r\n",
    "\r\n",
    "#   Attributes split by type\r\n",
    "num_float = ['sales_usd', 'conversion_rate']\r\n",
    "cat = [\"clock\", \"memory_dim\", 'id_ram', 'time_code', 'brand', 'ram_model', 'memory_type', 'vendor', 'continent', 'country', 'region', \"currency\"]\r\n",
    "\r\n",
    "#   per vedere i valori distinti - USARE ATTRIBUTI CATEGORICI\r\n",
    "for col in cat: \r\n",
    "    print(\"\\nDistinct values in \" + col + \" : \\t\", df_dropped[col].unique())\r\n",
    "\r\n",
    "\r\n",
    "# --------------------------- vs_num dataset\r\n",
    "# conversion \r\n",
    "for x in vs_num:\r\n",
    "    vs_num[x] = vs_num[x].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\r\n",
    "temp_list = vs_num.columns\r\n",
    "vs_num_norm = vs_num.copy()\r\n",
    "vs_num_norm[temp_list] = scaler.fit_transform(vs_num[temp_list].values)\r\n",
    "#print(vs_num_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best value of k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = []\r\n",
    "SIL = []\r\n",
    "SEP = []\r\n",
    "CAL = []\r\n",
    "\r\n",
    "for k in range(2, 13):        \r\n",
    "    k_means = cluster.KMeans(n_clusters=k, max_iter=100, random_state=1)   \r\n",
    "    cluster_labels = k_means.fit_predict(vs_num_norm)\r\n",
    "    k_means = k_means.fit(vs_num_norm)\r\n",
    "    labels = k_means.labels_\r\n",
    "    centroids = k_means.cluster_centers_\r\n",
    "\r\n",
    "    SSE.append(k_means.inertia_)\r\n",
    "    SIL.append(silhouette_score(vs_num_norm, cluster_labels, metric = 'euclidean'))\r\n",
    "    print(\"For n_clusters =\", k,\r\n",
    "          \"The average silhouette_score is :\", silhouette_score(vs_num_norm, cluster_labels, metric = 'euclidean'))    \r\n",
    "    SEP.append(davies_bouldin_score(vs_num_norm, cluster_labels))\r\n",
    "    CAL.append(calinski_harabasz_score(vs_num_norm, cluster_labels))\r\n",
    "\r\n",
    "plt.plot(range(2, 13), SSE)\r\n",
    "plt.xlabel('Number of Clusters')\r\n",
    "plt.ylabel('SSE')\r\n",
    "plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\sse.jpg\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "plt.plot(range(2, 13), SIL)\r\n",
    "plt.xlabel('Number of Clusters')\r\n",
    "plt.ylabel('Silhouette Score')\r\n",
    "plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\silhouette.jpg\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "plt.plot(range(2, 13), SEP)\r\n",
    "plt.xlabel('Number of Clusters')\r\n",
    "plt.ylabel('Davies Bouldin Index')\r\n",
    "plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\davies-bouldin.jpg\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "plt.plot(range(2, 13), CAL)\r\n",
    "plt.xlabel('Number of Clusters')\r\n",
    "plt.ylabel('Calinski-Harabasz Index')\r\n",
    "plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\calinski.jpg\")\r\n",
    "plt.show()\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- K = 3 --------------------\r\n",
    "k_means = cluster.KMeans(n_clusters=3, max_iter=100, random_state=1)   \r\n",
    "cluster_labels = k_means.fit_predict(vs_num_norm)\r\n",
    "k_means = k_means.fit(vs_num_norm)\r\n",
    "labels = k_means.labels_\r\n",
    "centroids = k_means.cluster_centers_\r\n",
    "        \r\n",
    "print(np.unique(labels, return_counts=True))\r\n",
    "\r\n",
    "\"\"\"for x in vs_num_norm.columns:\r\n",
    "    for y in vs_num_norm.columns:\r\n",
    "        for z in vs_num_norm.columns:\r\n",
    "            if (x != y) & (x != z) & (y != z):\r\n",
    "                plt.figure(figsize=(20, 15))\r\n",
    "                fig = go.Figure(data=[go.Scatter3d(\r\n",
    "                    x=vs_num_norm[x],\r\n",
    "                    y=vs_num_norm[y],\r\n",
    "                    z=vs_num_norm[z],\r\n",
    "                    mode='markers',\r\n",
    "                    marker=dict(\r\n",
    "                        size=6,\r\n",
    "                        color=labels, \r\n",
    "                        opacity=1\r\n",
    "                    )\r\n",
    "                )])\r\n",
    "                # tight layout\r\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\r\n",
    "                                scene = dict(\r\n",
    "                                    xaxis_title= x,\r\n",
    "                                    yaxis_title= y,\r\n",
    "                                    zaxis_title= z)\r\n",
    "                                )\r\n",
    "                #fig.write_image(dir + \"\\\\Clustering\\\\KMeans\\\\3d-after-kmeans.jpg\")\r\n",
    "                fig.show()\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "plt.figure(figsize=(30, 12))\r\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\r\n",
    "plt.xticks(range(0, len(vs_num_norm)), vs_num_norm, fontsize=20)\r\n",
    "for i in range(0, len(centroids)):\r\n",
    "    plt.plot(centroids[i], marker='o', label='Cluster %s' % i)\r\n",
    "plt.legend(fontsize=18)\r\n",
    "plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-parallel-coordinates.jpg\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "for x in vs_num_norm.columns:\r\n",
    "        plt.plot(legend='None')\r\n",
    "        var_val_xt = pd.crosstab(labels, vs_num[x])\r\n",
    "        var_val_xt_pct = \\\r\n",
    "            var_val_xt.div(var_val_xt.sum(1).astype('float'), axis=0)\r\n",
    "        var_val_xt_pct.plot(kind='bar', stacked=True, figsize = (20,15), fontsize=(4))\r\n",
    "        plt.title(x + ' by clusters')\r\n",
    "        plt.ylabel(x)\r\n",
    "        plt.xlabel('Clusters')\r\n",
    "        plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-crosstab-clustersby\" + x + \".jpg\")\r\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------- K = 9 --------------------\r\n",
    "k_means = cluster.KMeans(n_clusters=9, max_iter=100, random_state=1)   \r\n",
    "cluster_labels = k_means.fit_predict(vs_num_norm)\r\n",
    "k_means = k_means.fit(vs_num_norm)\r\n",
    "labels = k_means.labels_\r\n",
    "centroids = k_means.cluster_centers_\r\n",
    "        \r\n",
    "print(np.unique(labels, return_counts=True))\r\n",
    "\r\n",
    "\"\"\"for x in vs_num_norm.columns:\r\n",
    "    for y in vs_num_norm.columns:\r\n",
    "        for z in vs_num_norm.columns:\r\n",
    "            if (x != y) & (x != z) & (y != z):\r\n",
    "                plt.figure(figsize=(20, 15))\r\n",
    "                fig = go.Figure(data=[go.Scatter3d(\r\n",
    "                    x=vs_num_norm[x],\r\n",
    "                    y=vs_num_norm[y],\r\n",
    "                    z=vs_num_norm[z],\r\n",
    "                    mode='markers',\r\n",
    "                    marker=dict(\r\n",
    "                        size=6,\r\n",
    "                        color=labels, \r\n",
    "                        opacity=1\r\n",
    "                    )\r\n",
    "                )])\r\n",
    "                # tight layout\r\n",
    "                fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\r\n",
    "                                scene = dict(\r\n",
    "                                    xaxis_title= x,\r\n",
    "                                    yaxis_title= y,\r\n",
    "                                    zaxis_title= z)\r\n",
    "                                )\r\n",
    "                #fig.write_image(dir + \"\\\\Clustering\\\\KMeans\\\\3d-after-kmeans.jpg\")\r\n",
    "                fig.show()\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "plt.figure(figsize=(30, 12))\r\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\r\n",
    "plt.xticks(range(0, len(vs_num_norm)), vs_num_norm, fontsize=20)\r\n",
    "for i in range(0, len(centroids)):\r\n",
    "    plt.plot(centroids[i], marker='o', label='Cluster %s' % i)\r\n",
    "plt.legend(fontsize=18)\r\n",
    "plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-parallel-coordinates.jpg\")\r\n",
    "plt.show()\r\n",
    "\r\n",
    "for x in vs_num_norm.columns:\r\n",
    "        plt.plot(legend='None')\r\n",
    "        var_val_xt = pd.crosstab(labels, vs_num[x])\r\n",
    "        var_val_xt_pct = \\\r\n",
    "            var_val_xt.div(var_val_xt.sum(1).astype('float'), axis=0)\r\n",
    "        var_val_xt_pct.plot(kind='bar', stacked=True, figsize = (20,15), fontsize=(4))\r\n",
    "        plt.title(x + ' by clusters')\r\n",
    "        plt.ylabel(x)\r\n",
    "        plt.xlabel('Clusters')\r\n",
    "        plt.savefig(dir + \"\\\\Clustering\\\\KMeans\\\\kmeans-crosstab-clustersby\" + x + \".jpg\")\r\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Hierarchical Clustering (Ward method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison and evaluation of the different clustering via internal metrics\r\n",
    "\r\n",
    "Regarding the Sum of Squared Errors (SSE): - a decrease in the SSE value proportional to\r\n",
    "the number of clusters is an expected behavior, therefore opting directly for the clustering with the\r\n",
    "lowest sum of squared distances may not be a worthwhile decision.\r\n",
    "Regarding the Davies Bouldini Index: - a lower Davies-Bouldin index relates to a model with\r\n",
    "better separation between the clusters and, in this regard, the clustering with k equals to 2 seems\r\n",
    "to present the best separation among its clusters.\r\n",
    "Regarding the Silhouette Score: - a higher value for the Silhouette Coefficient relates to a model\r\n",
    "with better defined clusters, in this regard, the clustering with k equals to 2 presents the best\r\n",
    "score.\r\n",
    "Regarding the Calinski-Harabasz Index: - similarly to the Silhouette Coefficient, a higher value\r\n",
    "for the Calinski-Harabasz score relates to a model with better defined clusters, in this regard, the\r\n",
    "clustering with k equals to 2 seems to present the best defined clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallel coordinates and radar plot to show most influential attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparison results with external/internal indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by density-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------  DBSCAN-------------\r\n",
    "\r\n",
    "#----knee method--------\r\n",
    "dist = pdist(df_norm[norm_attr], 'euclidean')\r\n",
    "dist = squareform(dist)\r\n",
    "for i in range(2, 10):\r\n",
    "    k = math.pow(2, i)\r\n",
    "    kth_distances = []\r\n",
    "    for d in dist:\r\n",
    "        index_kth_distance = np.argsort(d)[round(k)]\r\n",
    "        kth_distances.append(d[index_kth_distance])\r\n",
    "    plt.plot(range(0, len(kth_distances)), sorted(kth_distances))\r\n",
    "plt.ylabel('Dist eps', fontsize=18)\r\n",
    "plt.xlabel('Sorted distances', fontsize=18)\r\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\r\n",
    "plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\DBSCAN\\\\eps.jpg')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "res = []\r\n",
    "eps = []\r\n",
    "p=0.35\r\n",
    "for i in range(0, 6):    #0.35-0.65\r\n",
    "    eps.append(p)\r\n",
    "    p+=0.05\r\n",
    "    \r\n",
    "for i in eps:\r\n",
    "    for n in range (7, df.shape[1]*2):     #2000rows -> ln= 7\r\n",
    "        dbscan = DBSCAN(eps=i, min_samples=n).fit(df_norm[norm_attr])\r\n",
    "        core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\r\n",
    "        core_samples_mask[dbscan.core_sample_indices_] = True\r\n",
    "        #remove -1 for outliers\r\n",
    "        #dbscan.labels_ = set([label for label in dbscan.labels_ if label >= 0])\r\n",
    "        if len(np.unique(dbscan.labels_, return_counts=True)[0]) <= 1:\r\n",
    "            continue\r\n",
    "        print(np.unique(dbscan.labels_, return_counts=True))\r\n",
    "        \r\n",
    "        res.append({\r\n",
    "            'label': len(np.unique(dbscan.labels_, return_counts=True)[0])-1,\r\n",
    "            'sil': silhouette_score(df_norm[norm_attr], dbscan.labels_),\r\n",
    "            'ms': n,\r\n",
    "            'eps': i\r\n",
    "        })\r\n",
    "\r\n",
    "\r\n",
    "#print(*res, sep = \"\\n\")   #better values are: label=>4, sil(max)=-0.25, ms=18or20, eps=0.35\r\n",
    "\r\n",
    "dbscan = DBSCAN(eps=0.35, min_samples=20).fit(df_norm[norm_attr])\r\n",
    "core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\r\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True\r\n",
    "print(\"Number of cluster from DBSCAN is: \", len(np.unique(dbscan.labels_, return_counts=True)[0])-1)\r\n",
    "print(np.unique(dbscan.labels_, return_counts=True))\r\n",
    "plt.figure(figsize=(18, 10))\r\n",
    "plt.scatter(df['pc'], df['clock_speed'], c=dbscan.labels_)\r\n",
    "plt.tick_params(axis='both', which='major', labelsize=22)\r\n",
    "plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\DBSCAN\\\\dbscan-scatter.jpg')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "x = 'clock_speed'\r\n",
    "y = 'n_cores'\r\n",
    "z = 'price_range'\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(16,12))\r\n",
    "fig = go.Figure(data=[go.Scatter3d(\r\n",
    "    x=df_norm[x],\r\n",
    "    y=df_norm[y],\r\n",
    "    z=df_norm[z],\r\n",
    "    mode='markers',\r\n",
    "    marker=dict(\r\n",
    "        size=5,\r\n",
    "        color=dbscan.labels_, \r\n",
    "        opacity=1\r\n",
    "    )\r\n",
    ")])\r\n",
    "# tight layout\r\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\r\n",
    "            scene = dict(\r\n",
    "                xaxis_title= x,\r\n",
    "                yaxis_title= y,\r\n",
    "                zaxis_title= z)\r\n",
    "            )\r\n",
    "fig.write_image('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\DBSCAN\\\\3d-after-dbscan.jpg')\r\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------Hierarchical--------------------------------\r\n",
    "\r\n",
    "#-----dendrogram----\r\n",
    "linkages = ['ward', 'complete', 'average', 'single']\r\n",
    "metrics = ['euclidean', 'manhattan']\r\n",
    "\r\n",
    "for i in linkages:\r\n",
    "    plt.title('Link: ' + i)\r\n",
    "    if i == 'ward':\r\n",
    "        plt.axhline(y=6, color='r', linestyle='--')\r\n",
    "    dist = pdist(df_norm[norm_attr], metric='euclidean')\r\n",
    "    link = linkage(dist, method=i, metric='euclidean')\r\n",
    "    res1 = dendrogram(link, color_threshold=7, truncate_mode='lastp', orientation='top',\r\n",
    "                      distance_sort='descending', show_leaf_counts=True)\r\n",
    "    plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\dendrogram-linkages-' + i + '.jpg')\r\n",
    "    plt.show()\r\n",
    "    \r\n",
    "\r\n",
    "res2 = []\r\n",
    "for k in range(3, 14):\r\n",
    "    for l in linkages:\r\n",
    "        for metr in metrics:\r\n",
    "            if l == 'ward' and metr != 'euclidean':\r\n",
    "                continue\r\n",
    "            hiera = AgglomerativeClustering(n_clusters=k, affinity=metr, linkage=l).fit(df_norm[norm_attr])\r\n",
    "            hist, bins = np.histogram(hiera.labels_, bins=range(0, len(set(hiera.labels_)) + 1))\r\n",
    "            res2.append({\r\n",
    "                #'hiera': hiera,\r\n",
    "                'labels': len(np.unique(hiera.labels_, return_counts=True)[0]), #dict(zip(bins, hist)),\r\n",
    "                'n_clusters': k,\r\n",
    "                'sil': silhouette_score(df_norm[norm_attr], hiera.labels_, metric = metr),\r\n",
    "                'link': l,\r\n",
    "                'metric': metr\r\n",
    "            })\r\n",
    "\r\n",
    "print(*res2, sep = \"\\n\")\r\n",
    "\r\n",
    "for i in df.columns:\r\n",
    "    for z in df.columns:\r\n",
    "        if i == z:\r\n",
    "            continue\r\n",
    "        plt.figure(figsize=(18, 10))\r\n",
    "        plt.scatter(df[i], df[z], c=hiera.labels_, cmap='rainbow')\r\n",
    "        plt.tick_params(axis='both', which='major', labelsize=22)\r\n",
    "        plt.xlabel(i)\r\n",
    "        plt.ylabel(z)\r\n",
    "        plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-scatter-'+i +'-' +z + '.jpg')\r\n",
    "        plt.show()\r\n",
    "\r\n",
    "x = 'clock_speed'\r\n",
    "y = 'n_cores'\r\n",
    "z = 'price_range'\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(16,12))\r\n",
    "fig = go.Figure(data=[go.Scatter3d(\r\n",
    "    x=df_norm[x],\r\n",
    "    y=df_norm[y],\r\n",
    "    z=df_norm[z],\r\n",
    "    mode='markers',\r\n",
    "    marker=dict(\r\n",
    "        size=5,\r\n",
    "        color=hiera.labels_, \r\n",
    "        opacity=1\r\n",
    "    )\r\n",
    ")])\r\n",
    "# tight layout\r\n",
    "fig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\r\n",
    "            scene = dict(\r\n",
    "                xaxis_title= x,\r\n",
    "                yaxis_title= y,\r\n",
    "                zaxis_title= z)\r\n",
    "            )\r\n",
    "fig.write_image('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\3d-after-hiera.jpg')\r\n",
    "fig.show()\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "x = range(3, 14)\r\n",
    "fig, ax = plt.subplots()\r\n",
    "y1 = [a for a in res2 if a['link'] == 'single' and a['metric'] == 'euclidean']\r\n",
    "y2 = [a for a in res2 if a['link'] == 'ward' and a['metric'] == 'euclidean']\r\n",
    "y3 = [a for a in res2 if a['link'] == 'complete' and a['metric'] == 'euclidean']\r\n",
    "y4 = [a for a in res2 if a['link'] == 'average' and a['metric'] == 'euclidean']\r\n",
    "ys = [y1, y2, y3, y4]\r\n",
    "for e in ys:\r\n",
    "    l = e[0]['link']\r\n",
    "    ax.plot(x, [s['sil'] for s in e], label=l)\r\n",
    "ax.set_title('Metric: Euclidean')\r\n",
    "plt.legend(fontsize=10)\r\n",
    "plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-cluster-plot.jpg')\r\n",
    "plt.show()\r\n",
    "\r\n",
    "fig, ax = plt.subplots()\r\n",
    "y1 = [a for a in res2 if a['link'] == 'single' and a['metric'] == 'manhattan']                               \r\n",
    "y3 = [a for a in res2 if a['link'] == 'complete' and a['metric'] == 'manhattan']\r\n",
    "y4 = [a for a in res2 if a['link'] == 'average' and a['metric'] == 'manhattan']\r\n",
    "ys = [y1, y3, y4]\r\n",
    "for e in ys:\r\n",
    "    l = e[0]['link']\r\n",
    "    ax.plot(x, [s['sil'] for s in e], label=l)\r\n",
    "ax.set_title('Metric: Manhattan')\r\n",
    "plt.legend(fontsize=10)\r\n",
    "plt.savefig('D:\\\\Dropbox\\\\Scuola\\\\Pisa\\\\Anno2\\\\Data Mining\\\\Esame\\\\Pratica\\\\python\\\\Clustering\\\\Hierarchical\\\\hiera-manhattan-plot.jpg')\r\n",
    "plt.show()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (OPTIONAL) Alternative clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the library: https://github.com/annoviko/pyclustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############################################# END ######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FUTURE CONSIDERATIONS\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
